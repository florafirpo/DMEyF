{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "330051ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree,  _tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\n",
    "import lightgbm as lgb\n",
    "from sklearn.impute import SimpleImputer\n",
    "import gc\n",
    "from joblib import Parallel, delayed\n",
    "from pathlib import Path\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_slice, plot_contour\n",
    "\n",
    "from time import time\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e652414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEMILLAS = [550007, 550019, 550031, 550033, 550047]\n",
    "\n",
    "mes_train = 202103, 202102, 202101\n",
    "mes_test = 202104\n",
    "mes_kaggle = 202106\n",
    "ganancia_acierto = 780000\n",
    "costo_estimulo = 20000\n",
    "# ====================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39165b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Que archivos son? esos meses seleccionados SIN SLOPE\n",
    "#df_train.write_csv(\"data/df_train_01_02_03.csv\")\n",
    "#df_test.write_csv(\"data/df_test_04.csv\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2a3b167",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train = pd.read_csv(\"data/df_train_fe.csv\")\n",
    "#df_test = pd.read_csv(\"data/df_test_04.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "470c4533",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 14.9 MiB for an array with shape (4, 486791) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_train = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/df_train_fe_sinslope.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Flor\\Documents\\UBA\\DMEyF\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Flor\\Documents\\UBA\\DMEyF\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Flor\\Documents\\UBA\\DMEyF\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1968\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1965\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1966\u001b[39m         new_col_dict = col_dict\n\u001b[32m-> \u001b[39m\u001b[32m1968\u001b[39m     df = \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_col_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1973\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1975\u001b[39m     \u001b[38;5;28mself\u001b[39m._currow += new_rows\n\u001b[32m   1976\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Flor\\Documents\\UBA\\DMEyF\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    772\u001b[39m     mgr = \u001b[38;5;28mself\u001b[39m._init_mgr(\n\u001b[32m    773\u001b[39m         data, axes={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: index, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: columns}, dtype=dtype, copy=copy\n\u001b[32m    774\u001b[39m     )\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    777\u001b[39m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     mgr = \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma.MaskedArray):\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Flor\\Documents\\UBA\\DMEyF\\venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[39m, in \u001b[36mdict_to_mgr\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    500\u001b[39m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[32m    501\u001b[39m         arrays = [x.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Flor\\Documents\\UBA\\DMEyF\\venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:152\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    149\u001b[39m axes = [columns, index]\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33mblock\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_block_manager_from_column_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrefs\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Flor\\Documents\\UBA\\DMEyF\\venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2139\u001b[39m, in \u001b[36mcreate_block_manager_from_column_arrays\u001b[39m\u001b[34m(arrays, axes, consolidate, refs)\u001b[39m\n\u001b[32m   2121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_block_manager_from_column_arrays\u001b[39m(\n\u001b[32m   2122\u001b[39m     arrays: \u001b[38;5;28mlist\u001b[39m[ArrayLike],\n\u001b[32m   2123\u001b[39m     axes: \u001b[38;5;28mlist\u001b[39m[Index],\n\u001b[32m   (...)\u001b[39m\u001b[32m   2135\u001b[39m     \u001b[38;5;66;03m# These last three are sufficient to allow us to safely pass\u001b[39;00m\n\u001b[32m   2136\u001b[39m     \u001b[38;5;66;03m#  verify_integrity=False below.\u001b[39;00m\n\u001b[32m   2138\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2139\u001b[39m         blocks = \u001b[43m_form_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2140\u001b[39m         mgr = BlockManager(blocks, axes, verify_integrity=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   2141\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Flor\\Documents\\UBA\\DMEyF\\venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2212\u001b[39m, in \u001b[36m_form_blocks\u001b[39m\u001b[34m(arrays, consolidate, refs)\u001b[39m\n\u001b[32m   2209\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(dtype.type, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[32m   2210\u001b[39m     dtype = np.dtype(\u001b[38;5;28mobject\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2212\u001b[39m values, placement = \u001b[43m_stack_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtup_block\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2213\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_dtlike:\n\u001b[32m   2214\u001b[39m     values = ensure_wrapped_if_datetimelike(values)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Flor\\Documents\\UBA\\DMEyF\\venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2252\u001b[39m, in \u001b[36m_stack_arrays\u001b[39m\u001b[34m(tuples, dtype)\u001b[39m\n\u001b[32m   2249\u001b[39m first = arrays[\u001b[32m0\u001b[39m]\n\u001b[32m   2250\u001b[39m shape = (\u001b[38;5;28mlen\u001b[39m(arrays),) + first.shape\n\u001b[32m-> \u001b[39m\u001b[32m2252\u001b[39m stacked = np.empty(shape, dtype=dtype)\n\u001b[32m   2253\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, arr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arrays):\n\u001b[32m   2254\u001b[39m     stacked[i] = arr\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 14.9 MiB for an array with shape (4, 486791) and data type int64"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"data/df_train_fe_sinslope.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734a6cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"data/df_test_fe_sinslope.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da8cf600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(486791, 803)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31260188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163418, 803)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e37906fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_03_continua = pd.read_csv(\"data/df_train_01_02_03.csv\")\n",
    "# #Selecciono solo las 202103 en foto_es y continua en clase_ternaria\n",
    "# df_train_03_continua = df_train_03_continua[(df_train_03_continua['foto_mes'] == 202103) & (df_train_03_continua['clase_ternaria'] == 'CONTINUA')]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6db8be6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardar en csv\n",
    "#df_train_03_continua.to_csv(\"data/df_train_03_continua.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adca8f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar los meses deseados\n",
    "# df_train = df_trainc\n",
    "\n",
    "\n",
    "# #Guardar en CSV\n",
    "# df_train.to_csv(\"df_train_c12.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd2aebea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Liberar memoria antes de empezar\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ab0f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kaggle = pd.read_csv(\"data/df_kaggle_fe_sinslope.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d3424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163418, 801)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7cd81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(197886, 801)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44314d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164313, 801)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_kaggle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36daa60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#tiene mi df_train la columna \"clase_ternaria\"?\n",
    "print(\"clase_ternaria\" in df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6799687a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset despu√©s de downsampling: (197886, 801)\n",
      "CONTINUA: 192602\n",
      "NO CONTINUA: 5284\n"
     ]
    }
   ],
   "source": [
    "#Downsampling de la clase Continua en df_train\n",
    "# ===== DOWNSAMPLING EN DF_TRAIN (ANTES DEL SPLIT) =====\n",
    "# Identificar √≠ndices de cada clase\n",
    "idx_continua = df_train[df_train['clase_ternaria'] == 'CONTINUA'].index\n",
    "idx_no_continua = df_train[df_train['clase_ternaria'] != 'CONTINUA'].index\n",
    "\n",
    "# Definir qu√© porcentaje de CONTINUA quer√©s mantener\n",
    "porcentaje_continua = 0.40  # üî• Mantener 20 % de CONTINUA\n",
    "\n",
    "# Calcular cu√°ntos samples mantener\n",
    "n_samples_continua = int(len(idx_continua) * porcentaje_continua)\n",
    "\n",
    "# Hacer el subsample aleatorio de CONTINUA\n",
    "np.random.seed(42)\n",
    "idx_continua_subsampled = np.random.choice(idx_continua, size=n_samples_continua, replace=False)\n",
    "\n",
    "# Combinar √≠ndices\n",
    "idx_df_balanced = np.concatenate([idx_continua_subsampled, idx_no_continua])\n",
    "\n",
    "# Aplicar el subsample a df_train\n",
    "df_train = df_train.loc[idx_df_balanced]\n",
    "\n",
    "print(f\"Dataset despu√©s de downsampling: {df_train.shape}\")\n",
    "print(f\"CONTINUA: {(df_train['clase_ternaria']=='CONTINUA').sum()}\")\n",
    "print(f\"NO CONTINUA: {(df_train['clase_ternaria']!='CONTINUA').sum()}\")\n",
    "# =======================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b9a2aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Liberar memoria antes de empezar\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a08b7d",
   "metadata": {},
   "source": [
    "##Optimizaci√≥n con LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38450efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['clase_peso'] = 1.0\n",
    "\n",
    "df_train.loc[df_train['clase_ternaria'] == 'BAJA+2', 'clase_peso'] = 1.00002\n",
    "df_train.loc[df_train['clase_ternaria'] == 'BAJA+1', 'clase_peso'] = 1.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5edea37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (71109, 802) (71109,) (71109,)\n",
      "Validation: (146038, 802) (146038,) (146038,)\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separar X e y\n",
    "X = df_train.drop([\"clase_ternaria\", \"clase_peso\"], axis=1)  # ‚úÖ Sacamos tambi√©n clase_peso\n",
    "y = df_train[\"clase_ternaria\"]\n",
    "pesos = df_train[\"clase_peso\"]  # ‚úÖ Guardamos los pesos\n",
    "\n",
    "# Binarizar y\n",
    "y_binaria = (y != \"CONTINUA\").astype(int)\n",
    "\n",
    "# Split 70/30 (ahora incluimos los pesos)\n",
    "X_train, X_val, y_train, y_val, pesos_train, pesos_val = train_test_split(\n",
    "    X, y_binaria, pesos,  # ‚úÖ Separamos X, y Y pesos\n",
    "    train_size=0.7,\n",
    "    random_state=42,\n",
    "    stratify=y_binaria\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, y_train.shape, pesos_train.shape)\n",
    "print(\"Validation:\", X_val.shape, y_val.shape, pesos_val.shape)\n",
    "\n",
    "# Ahora en el Dataset:\n",
    "train_data = lgb.Dataset(X_train,\n",
    "                          label=y_train,\n",
    "                          weight=pesos_train  # ‚úÖ Usamos los pesos del train\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cc7fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separar X e y\n",
    "X = df_train.drop([\"clase_ternaria\", \"clase_peso\"], axis=1)  # ‚úÖ Sacamos tambi√©n clase_peso\n",
    "y = df_train[\"clase_ternaria\"]\n",
    "pesos = df_train[\"clase_peso\"]  # ‚úÖ Guardamos los pesos\n",
    "\n",
    "# Binarizar y\n",
    "y_binaria = (y != \"CONTINUA\").astype(int)\n",
    "\n",
    "# Split 70/30 (ahora incluimos los pesos)\n",
    "X_train, X_val, y_train, y_val, pesos_train, pesos_val = train_test_split(\n",
    "    X, y_binaria, pesos,  # ‚úÖ Separamos X, y Y pesos\n",
    "    train_size=0.7,\n",
    "    random_state=42,\n",
    "    stratify=y_binaria\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, y_train.shape, pesos_train.shape)\n",
    "print(\"Validation:\", X_val.shape, y_val.shape, pesos_val.shape)\n",
    "\n",
    "# Ahora en el Dataset:\n",
    "train_data = lgb.Dataset(X_train,\n",
    "                          label=y_train,\n",
    "                          weight=pesos_train  # ‚úÖ Usamos los pesos del train\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63f3d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ganancia_prob(y_pred, data):\n",
    "  weight = data.get_weight()\n",
    "  ganancia = np.where(weight == 1.00002, ganancia_acierto, 0) - np.where(weight < 1.00002, costo_estimulo, 0)\n",
    "  ganancia = ganancia[np.argsort(y_pred)[::-1]]\n",
    "  ganancia = np.cumsum(ganancia)  # ‚úÖ Bien\n",
    "  return 'gan_eval', np.max(ganancia), True\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7157293f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-31 16:48:31,608] Using an existing study with name 'exp_1_lgbm' instead of creating a new one.\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    num_leaves = trial.suggest_int('num_leaves', 8, 300)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.005, 0.1) # mas bajo, m√°s iteraciones necesita\n",
    "    min_data_in_leaf = trial.suggest_int('min_data_in_leaf', 50, 1500)\n",
    "    feature_fraction = trial.suggest_float('feature_fraction', 0.1, 1.0)\n",
    "    bagging_fraction = trial.suggest_float('bagging_fraction', 0.1, 1.0),\n",
    "\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'custom',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'first_metric_only': True,\n",
    "        'boost_from_average': True,\n",
    "        'feature_pre_filter': False,\n",
    "        'max_bin': 31,\n",
    "        'num_leaves': num_leaves,\n",
    "        'learning_rate': learning_rate,\n",
    "        'min_data_in_leaf': min_data_in_leaf,\n",
    "        'feature_fraction': feature_fraction,\n",
    "        'bagging_fraction': bagging_fraction,\n",
    "        'seed': SEMILLAS[0],\n",
    "        'verbose': -1\n",
    "    }\n",
    "    train_data = lgb.Dataset(X_train,\n",
    "                              label=y_train, # eligir la clase\n",
    "                              weight=pesos_train\n",
    "                              )\n",
    "    cv_results = lgb.cv(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=1000, # modificar, subit y subir... y descomentar la l√≠nea inferior\n",
    "        #early_stopping_rounds= int(50 + 5 / learning_rate),\n",
    "        feval=ganancia_prob,\n",
    "        stratified=True,\n",
    "        nfold=4,\n",
    "        seed=SEMILLAS[0],\n",
    "        callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=int(50 + 5/learning_rate), verbose=False),\n",
    "                lgb.log_evaluation(period=200),\n",
    "                ]\n",
    "    )\n",
    "    max_gan = max(cv_results['valid gan_eval-mean'])\n",
    "    best_iter = cv_results['valid gan_eval-mean'].index(max_gan) + 1\n",
    "\n",
    "    # Guardamos cual es la mejor iteraci√≥n del modelo\n",
    "    trial.set_user_attr(\"best_iter\", best_iter)\n",
    "\n",
    "    return max_gan \n",
    "# Al final de objective()\n",
    "gc.collect()\n",
    "\n",
    "#guardar el archivo en mi carpeta data\n",
    "storage_name = \"sqlite:///data/optimization_lgbm.db\"\n",
    "\n",
    "study_name = \"exp_1_lgbm\"\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=study_name,\n",
    "    storage=storage_name,\n",
    "    load_if_exists=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1bddf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid's gan_eval: 3.12915e+08 + 1.32316e+07\n",
      "[400]\tvalid's gan_eval: 3.1683e+08 + 1.34441e+07\n",
      "[600]\tvalid's gan_eval: 3.1726e+08 + 1.34812e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-31 16:50:37,835] Trial 2 finished with value: 318155000.0 and parameters: {'num_leaves': 239, 'learning_rate': 0.09675632362583497, 'min_data_in_leaf': 343, 'feature_fraction': 0.14845557131157736, 'bagging_fraction': 0.9603963995110338}. Best is trial 0 with value: 1287080000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid's gan_eval: 3.1969e+08 + 1.63468e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-10-31 16:52:36,198] Trial 3 failed with parameters: {'num_leaves': 177, 'learning_rate': 0.09864195605252882, 'min_data_in_leaf': 423, 'feature_fraction': 0.6519206961918165, 'bagging_fraction': 0.2050592505151917} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Flor\\Documents\\UBA\\DMEyF\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Flor\\AppData\\Local\\Temp\\ipykernel_6464\\1685824414.py\", line 29, in objective\n",
      "    cv_results = lgb.cv(\n",
      "                 ^^^^^^^\n",
      "  File \"c:\\Users\\Flor\\Documents\\UBA\\DMEyF\\venv\\Lib\\site-packages\\lightgbm\\engine.py\", line 829, in cv\n",
      "    cvbooster.update(fobj=fobj)  # type: ignore[call-arg]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Flor\\Documents\\UBA\\DMEyF\\venv\\Lib\\site-packages\\lightgbm\\engine.py\", line 417, in handler_function\n",
      "    ret.append(getattr(booster, name)(*args, **kwargs))\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Flor\\Documents\\UBA\\DMEyF\\venv\\Lib\\site-packages\\lightgbm\\basic.py\", line 4155, in update\n",
      "    _LIB.LGBM_BoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2025-10-31 16:52:36,207] Trial 3 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# subir subir\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Flor\\Documents\\UBA\\DMEyF\\venv\\Lib\\site-packages\\optuna\\study\\study.py:451\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    349\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    350\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    357\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    358\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    359\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    360\u001b[39m \n\u001b[32m    361\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    449\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Flor\\Documents\\UBA\\DMEyF\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:62\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     75\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Flor\\Documents\\UBA\\DMEyF\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:159\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    156\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Flor\\Documents\\UBA\\DMEyF\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:247\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    240\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    243\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    244\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    245\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    246\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Flor\\Documents\\UBA\\DMEyF\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:196\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    198\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    199\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m      9\u001b[39m params = {\n\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mobjective\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmetric\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mcustom\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mverbose\u001b[39m\u001b[33m'\u001b[39m: -\u001b[32m1\u001b[39m\n\u001b[32m     24\u001b[39m }\n\u001b[32m     25\u001b[39m train_data = lgb.Dataset(X_train,\n\u001b[32m     26\u001b[39m                           label=y_train, \u001b[38;5;66;03m# eligir la clase\u001b[39;00m\n\u001b[32m     27\u001b[39m                           weight=pesos_train\n\u001b[32m     28\u001b[39m                           )\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m cv_results = \u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# modificar, subit y subir... y descomentar la l√≠nea inferior\u001b[39;49;00m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#early_stopping_rounds= int(50 + 5 / learning_rate),\u001b[39;49;00m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mganancia_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstratified\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnfold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEMILLAS\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m/\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperiod\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m max_gan = \u001b[38;5;28mmax\u001b[39m(cv_results[\u001b[33m'\u001b[39m\u001b[33mvalid gan_eval-mean\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     44\u001b[39m best_iter = cv_results[\u001b[33m'\u001b[39m\u001b[33mvalid gan_eval-mean\u001b[39m\u001b[33m'\u001b[39m].index(max_gan) + \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Flor\\Documents\\UBA\\DMEyF\\venv\\Lib\\site-packages\\lightgbm\\engine.py:829\u001b[39m, in \u001b[36mcv\u001b[39m\u001b[34m(params, train_set, num_boost_round, folds, nfold, stratified, shuffle, metrics, feval, init_model, fpreproc, seed, callbacks, eval_train_metric, return_cvbooster)\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[32m    819\u001b[39m     cb(\n\u001b[32m    820\u001b[39m         callback.CallbackEnv(\n\u001b[32m    821\u001b[39m             model=cvbooster,\n\u001b[32m   (...)\u001b[39m\u001b[32m    827\u001b[39m         )\n\u001b[32m    828\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[43mcvbooster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m    830\u001b[39m res = _agg_cv_result(cvbooster.eval_valid(feval))  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dataset_name, metric_name, metric_mean, _, metric_std_dev \u001b[38;5;129;01min\u001b[39;00m res:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Flor\\Documents\\UBA\\DMEyF\\venv\\Lib\\site-packages\\lightgbm\\engine.py:417\u001b[39m, in \u001b[36mCVBooster.__getattr__.<locals>.handler_function\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    415\u001b[39m ret = []\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m booster \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.boosters:\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     ret.append(\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbooster\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Flor\\Documents\\UBA\\DMEyF\\venv\\Lib\\site-packages\\lightgbm\\basic.py:4155\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, train_set, fobj)\u001b[39m\n\u001b[32m   4152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__set_objective_to_none:\n\u001b[32m   4153\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[33m\"\u001b[39m\u001b[33mCannot update due to null objective function.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4154\u001b[39m _safe_call(\n\u001b[32m-> \u001b[39m\u001b[32m4155\u001b[39m     \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4156\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4158\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4159\u001b[39m )\n\u001b[32m   4160\u001b[39m \u001b[38;5;28mself\u001b[39m.__is_predicted_cur_iter = [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.__num_dataset)]\n\u001b[32m   4161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished.value == \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "study.optimize(objective, n_trials=10) # subir subir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9a72c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CARGANDO ESTUDIO DE OPTUNA\n",
      "============================================================\n",
      "Estudio cargado: exp_301_lgbm\n",
      "N√∫mero de trials: 894\n",
      "Mejor valor: $1,785,980,000\n",
      "Mejores par√°metros: {'num_leaves': 147, 'learning_rate': 0.07055166350630575, 'min_data_in_leaf': 183, 'feature_fraction': 0.6992396968297147, 'bagging_fraction': 0.49046102060858093}\n",
      "\n",
      "============================================================\n",
      "GENERANDO GR√ÅFICOS INTERACTIVOS (PLOTLY)\n",
      "============================================================\n",
      "‚úÖ Guardado: data/graficos/optimization_history.html\n",
      "‚úÖ Guardado: data/graficos/param_importances.html\n",
      "‚úÖ Guardado: data/graficos/contour_leaves_lr.html\n",
      "‚úÖ Guardado: data/graficos/slice_plot.html\n",
      "‚úÖ Guardado: data/graficos/parallel_coordinate.html\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ============================================================\n",
    "# CARGAR ESTUDIO DE OPTUNA\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CARGANDO ESTUDIO DE OPTUNA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "storage_name = \"sqlite:///data/optimization_lgbm.db\"\n",
    "study_name = \"exp_301_lgbm\"\n",
    "\n",
    "study = optuna.load_study(\n",
    "    study_name=study_name,\n",
    "    storage=storage_name\n",
    ")\n",
    "\n",
    "print(f\"Estudio cargado: {study_name}\")\n",
    "print(f\"N√∫mero de trials: {len(study.trials)}\")\n",
    "print(f\"Mejor valor: ${study.best_value:,.0f}\")\n",
    "print(f\"Mejores par√°metros: {study.best_params}\")\n",
    "\n",
    "# ============================================================\n",
    "# OPCI√ìN 1: GR√ÅFICOS INTERACTIVOS CON PLOTLY (OPTUNA)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERANDO GR√ÅFICOS INTERACTIVOS (PLOTLY)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Historia de optimizaci√≥n\n",
    "fig = plot_optimization_history(study)\n",
    "fig.write_html(\"data/graficos/optimization_history.html\")\n",
    "print(\"‚úÖ Guardado: data/graficos/optimization_history.html\")\n",
    "\n",
    "# 2. Importancia de par√°metros\n",
    "fig = plot_param_importances(study)\n",
    "fig.write_html(\"data/graficos/param_importances.html\")\n",
    "print(\"‚úÖ Guardado: data/graficos/param_importances.html\")\n",
    "\n",
    "# 3. Gr√°fico de contorno (relaciones entre pares de par√°metros)\n",
    "from optuna.visualization import plot_contour\n",
    "fig = plot_contour(study, params=['num_leaves', 'learning_rate'])\n",
    "fig.write_html(\"data/graficos/contour_leaves_lr.html\")\n",
    "print(\"‚úÖ Guardado: data/graficos/contour_leaves_lr.html\")\n",
    "\n",
    "# 4. Slice plot (cada hiperpar√°metro vs ganancia)\n",
    "from optuna.visualization import plot_slice\n",
    "fig = plot_slice(study)\n",
    "fig.write_html(\"data/graficos/slice_plot.html\")\n",
    "print(\"‚úÖ Guardado: data/graficos/slice_plot.html\")\n",
    "\n",
    "# 5. Parallel coordinate plot\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "fig = plot_parallel_coordinate(study)\n",
    "fig.write_html(\"data/graficos/parallel_coordinate.html\")\n",
    "print(\"‚úÖ Guardado: data/graficos/parallel_coordinate.html\")\n",
    "\n",
    "# ============================================================\n",
    "# OPCI√ìN 2: GR√ÅFICO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2b7d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Obtener los mejores 10 trials\n",
    "best_trials = study.trials_dataframe().sort_values('value', ascending=False).head(10)\n",
    "\n",
    "# Opci√≥n 1: Guardar como lista de diccionarios (m√°s completo)\n",
    "mejores_params = []\n",
    "for i, trial in enumerate(study.best_trials[:10] if len(study.best_trials) >= 10 else study.best_trials):\n",
    "    mejores_params.append({\n",
    "        'rank': i + 1,\n",
    "        'trial_number': trial.number,\n",
    "        'value': trial.value,\n",
    "        'params': trial.params,\n",
    "        'best_iter': trial.user_attrs.get('best_iter', None)\n",
    "    })\n",
    "\n",
    "# Guardar en JSON\n",
    "with open('data/mejores_hiperparametros.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(mejores_params, f, indent=4, ensure_ascii=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10229d4c",
   "metadata": {},
   "source": [
    "Modelo 1. Predecir abril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f180540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PREPARANDO DATOS TRAIN\n",
      "============================================================\n",
      "Convirtiendo a numpy (float32)...\n",
      "X_train: (101585, 802), dtype: float32\n",
      "y_train_binaria: (101585,), dtype: int8\n",
      "Clase positiva: 5,284\n",
      "Clase negativa: 96,301\n",
      "Liberando df_train de memoria...\n",
      "‚úÖ Datos TRAIN preparados y df_train liberado\n",
      "\n",
      "============================================================\n",
      "PREPARANDO DATOS TEST\n",
      "============================================================\n",
      "X_test: (163418, 802), dtype: float32\n",
      "y_test: (163418,)\n",
      "Liberando df_test de memoria...\n",
      "‚úÖ Datos TEST preparados\n",
      "\n",
      "============================================================\n",
      "ENTRENAMIENTO CON TRAIN\n",
      "============================================================\n",
      "\n",
      "Cargando hiperpar√°metros desde data/mejores_hiperparametros.json...\n",
      "Trial 672\n",
      "Ganancia esperada en CV: $1,785,980,000\n",
      "\n",
      "Par√°metros del modelo:\n",
      "  objective: binary\n",
      "  metric: custom\n",
      "  boosting_type: gbdt\n",
      "  first_metric_only: True\n",
      "  boost_from_average: True\n",
      "  feature_pre_filter: False\n",
      "  max_bin: 31\n",
      "  num_leaves: 147\n",
      "  learning_rate: 0.07055166350630575\n",
      "  min_data_in_leaf: 147\n",
      "  feature_fraction: 0.6992396968297147\n",
      "  bagging_fraction: 0.49046102060858093\n",
      "  seed: 550007\n",
      "  verbose: 1\n",
      "\n",
      "Creando dataset con X_train: (101585, 802)\n",
      "Entrenando modelo con 737 iteraciones...\n",
      "[LightGBM] [Info] Number of positive: 5284, number of negative: 96301\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123526 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19964\n",
      "[LightGBM] [Info] Number of data points in the train set: 101585, number of used features: 802\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.052016 -> initscore=-2.902780\n",
      "[LightGBM] [Info] Start training from score -2.902780\n",
      "‚úÖ Modelo entrenado exitosamente\n",
      "\n",
      "============================================================\n",
      "EVALUANDO EN TEST\n",
      "============================================================\n",
      "Generando predicciones en X_test: (163418, 802)\n",
      "Calculando ganancia acumulada...\n",
      "\n",
      "üìä RESULTADOS EN TEST:\n",
      "  Ganancia m√°xima: $441,180,000\n",
      "  N¬∞ de env√≠os √≥ptimo: 10,661\n",
      "  Threshold √≥ptimo: 0.000335\n",
      "  % de la base: 6.52%\n",
      "\n",
      "üìà GANANCIA POR CANTIDAD DE ENV√çOS:\n",
      "  Top 1.0% (1,634 env√≠os): $201,720,000\n",
      "  Top 2.5% (4,085 env√≠os): $364,700,000\n",
      "  Top 5.0% (8,170 env√≠os): $425,400,000\n",
      "  Top 10.0% (16,341 env√≠os): $396,380,000\n",
      "\n",
      "üéØ CAPTURA DE BAJA+2:\n",
      "  Total BAJA+2: 1,131\n",
      "  Capturados: 818\n",
      "  Tasa: 72.33%\n",
      "\n",
      "‚úÖ Resultados guardados\n",
      "‚úÖ Modelo guardado\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "# ============================================================\n",
    "# PREPARAR DATOS DE TRAIN - OPTIMIZADO PARA MEMORIA\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PREPARANDO DATOS TRAIN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 2. Convertir a numpy inmediatamente (m√°s eficiente en memoria)\n",
    "print(\"Convirtiendo a numpy (float32)...\")\n",
    "X_train = df_train.drop([\"clase_ternaria\", \"clase_peso\"], axis=1).to_numpy().astype('float32')\n",
    "y_train = df_train[\"clase_ternaria\"].to_numpy()\n",
    "pesos_train = df_train[\"clase_peso\"].to_numpy().astype('float32')\n",
    "\n",
    "# Binarizar\n",
    "y_train_binaria = (y_train != \"CONTINUA\").astype('int8')\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, dtype: {X_train.dtype}\")\n",
    "print(f\"y_train_binaria: {y_train_binaria.shape}, dtype: {y_train_binaria.dtype}\")\n",
    "print(f\"Clase positiva: {y_train_binaria.sum():,}\")\n",
    "print(f\"Clase negativa: {(y_train_binaria == 0).sum():,}\")\n",
    "\n",
    "# Liberar df_train de la memoria\n",
    "print(\"Liberando df_train de memoria...\")\n",
    "del df_train, y_train\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Datos TRAIN preparados y df_train liberado\")\n",
    "\n",
    "# ============================================================\n",
    "# PREPARAR DATOS DE TEST\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPARANDO DATOS TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_test = df_test.drop(\"clase_ternaria\", axis=1).to_numpy().astype('float32')\n",
    "y_test = df_test[\"clase_ternaria\"].to_numpy()\n",
    "\n",
    "print(f\"X_test: {X_test.shape}, dtype: {X_test.dtype}\")\n",
    "print(f\"y_test: {y_test.shape}\")\n",
    "\n",
    "# Liberar df_test si no lo necesit√°s m√°s\n",
    "print(\"Liberando df_test de memoria...\")\n",
    "del df_test\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Datos TEST preparados\")\n",
    "\n",
    "# ============================================================\n",
    "# DEFINIR M√âTRICA PERSONALIZADA DE GANANCIA\n",
    "# ============================================================\n",
    "\n",
    "ganancia_acierto = 780000\n",
    "costo_estimulo = 20000\n",
    "\n",
    "def ganancia_lgb(y_pred, data):\n",
    "    \"\"\"M√©trica personalizada de ganancia para LightGBM.\"\"\"\n",
    "    weight = data.get_weight()\n",
    "    \n",
    "    # Calcular ganancia para cada predicci√≥n\n",
    "    ganancia = np.where(weight == 1.00002, ganancia_acierto, 0) - costo_estimulo\n",
    "    \n",
    "    # Ordenar por predicci√≥n descendente\n",
    "    indices = np.argsort(-y_pred)\n",
    "    ganancia_ordenada = ganancia[indices]\n",
    "    \n",
    "    # Calcular ganancia acumulada m√°xima\n",
    "    ganancia_acum = np.cumsum(ganancia_ordenada)\n",
    "    max_ganancia = np.max(ganancia_acum)\n",
    "    \n",
    "    return 'ganancia', max_ganancia, True\n",
    "\n",
    "# ============================================================\n",
    "# ENTRENAR CON TRAIN Y EVALUAR EN TEST\n",
    "# ============================================================\n",
    "\n",
    "def entrenar_modelo_test(mejores_params_path='data/mejores_hiperparametros_reg.json'):\n",
    "    \"\"\"Entrena el modelo con df_train usando la m√©trica de ganancia.\"\"\"\n",
    "    \n",
    "    print(f\"\\nCargando hiperpar√°metros desde {mejores_params_path}...\")\n",
    "    with open(mejores_params_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    config = data[0] if isinstance(data, list) else data\n",
    "    \n",
    "    print(f\"Trial {config['trial_number']}\")\n",
    "    print(f\"Ganancia esperada en CV: ${config['value']:,.0f}\")\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'custom',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'first_metric_only': True,\n",
    "        'boost_from_average': True,\n",
    "        'feature_pre_filter': False,\n",
    "        'max_bin': 31,\n",
    "        'num_leaves': config['params']['num_leaves'],\n",
    "        'learning_rate': config['params']['learning_rate'],\n",
    "        'min_data_in_leaf': 147,\n",
    "        'feature_fraction': config['params']['feature_fraction'],\n",
    "        'bagging_fraction': config['params']['bagging_fraction'],\n",
    "        'seed': SEMILLAS[0],\n",
    "        'verbose': 1\n",
    "    }\n",
    "    \n",
    "    print(\"\\nPar√°metros del modelo:\")\n",
    "    for key, value in params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Crear dataset (free_raw_data=True para ahorrar memoria)\n",
    "    print(f\"\\nCreando dataset con X_train: {X_train.shape}\")\n",
    "    train_data = lgb.Dataset(X_train,\n",
    "                             label=y_train_binaria,\n",
    "                             weight=pesos_train,\n",
    "                             free_raw_data=False)  # Cambiar a True si sigue fallando\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    print(f\"Entrenando modelo con {config['best_iter']} iteraciones...\")\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=config['best_iter'],\n",
    "        feval=ganancia_lgb,\n",
    "        callbacks=[lgb.log_evaluation(period=50)]\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Modelo entrenado exitosamente\")\n",
    "    \n",
    "    # Liberar train_data\n",
    "    del train_data\n",
    "    gc.collect()\n",
    "    \n",
    "    return model, config\n",
    "\n",
    "\n",
    "def calcular_ganancia_test(model, X_test, y_test):\n",
    "    \"\"\"Calcula la ganancia en TEST y encuentra el umbral √≥ptimo.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUANDO EN TEST\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Generando predicciones en X_test: {X_test.shape}\")\n",
    "    y_test_pred_prob = model.predict(X_test)\n",
    "    \n",
    "    indices_ordenados = np.argsort(-y_test_pred_prob)\n",
    "    y_test_sorted = y_test[indices_ordenados]\n",
    "    \n",
    "    print(\"Calculando ganancia acumulada...\")\n",
    "    ganancia_cum = np.cumsum([\n",
    "        (ganancia_acierto if y == \"BAJA+2\" else -costo_estimulo)\n",
    "        for y in y_test_sorted\n",
    "    ])\n",
    "    \n",
    "    max_ganancia_idx = np.argmax(ganancia_cum)\n",
    "    max_ganancia = ganancia_cum[max_ganancia_idx]\n",
    "    n_envios_optimo = max_ganancia_idx + 1\n",
    "    threshold_optimo = y_test_pred_prob[indices_ordenados[max_ganancia_idx]]\n",
    "    \n",
    "    print(f\"\\nüìä RESULTADOS EN TEST:\")\n",
    "    print(f\"  Ganancia m√°xima: ${max_ganancia:,.0f}\")\n",
    "    print(f\"  N¬∞ de env√≠os √≥ptimo: {n_envios_optimo:,}\")\n",
    "    print(f\"  Threshold √≥ptimo: {threshold_optimo:.6f}\")\n",
    "    print(f\"  % de la base: {n_envios_optimo / len(y_test) * 100:.2f}%\")\n",
    "    \n",
    "    print(\"\\nüìà GANANCIA POR CANTIDAD DE ENV√çOS:\")\n",
    "    for percentil in [0.01, 0.025, 0.05, 0.10]:\n",
    "        n = int(len(y_test) * percentil)\n",
    "        ganancia_n = ganancia_cum[n-1] if n > 0 else 0\n",
    "        print(f\"  Top {percentil*100:.1f}% ({n:,} env√≠os): ${ganancia_n:,.0f}\")\n",
    "    \n",
    "    n_baja2_total = (y_test == \"BAJA+2\").sum()\n",
    "    n_baja2_capturados = (y_test_sorted[:n_envios_optimo] == \"BAJA+2\").sum()\n",
    "    print(f\"\\nüéØ CAPTURA DE BAJA+2:\")\n",
    "    print(f\"  Total BAJA+2: {n_baja2_total:,}\")\n",
    "    print(f\"  Capturados: {n_baja2_capturados:,}\")\n",
    "    print(f\"  Tasa: {n_baja2_capturados / n_baja2_total * 100:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'ganancia_maxima': max_ganancia,\n",
    "        'n_envios_optimo': n_envios_optimo,\n",
    "        'threshold_optimo': threshold_optimo,\n",
    "        'ganancia_acumulada': ganancia_cum,\n",
    "        'probabilidades_ordenadas': y_test_pred_prob[indices_ordenados],\n",
    "        'y_test_sorted': y_test_sorted,\n",
    "        'n_baja2_capturados': n_baja2_capturados,\n",
    "        'n_baja2_total': n_baja2_total\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FLUJO COMPLETO\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENTRENAMIENTO CON TRAIN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_test, config = entrenar_modelo_test('data/mejores_hiperparametros.json')\n",
    "\n",
    "resultados_test = calcular_ganancia_test(model_test, X_test, y_test)\n",
    "\n",
    "with open('data/resultados_test.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'ganancia_maxima': float(resultados_test['ganancia_maxima']),\n",
    "        'n_envios_optimo': int(resultados_test['n_envios_optimo']),\n",
    "        'threshold_optimo': float(resultados_test['threshold_optimo']),\n",
    "        'n_baja2_capturados': int(resultados_test['n_baja2_capturados']),\n",
    "        'n_baja2_total': int(resultados_test['n_baja2_total'])\n",
    "    }, f, indent=4)\n",
    "\n",
    "print(\"\\n‚úÖ Resultados guardados\")\n",
    "\n",
    "model_test.save_model('data/modelo_train_test2.txt')\n",
    "print(\"‚úÖ Modelo guardado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e73bd47b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m importances = model_test.feature_importance()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m feature_names = \u001b[43mX\u001b[49m.columns  \u001b[38;5;66;03m# tomamos los nombres desde tu dataset original\u001b[39;00m\n\u001b[32m      4\u001b[39m importance_df = pd.DataFrame({\n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mFeature\u001b[39m\u001b[33m'\u001b[39m: feature_names,\n\u001b[32m      6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mImportance\u001b[39m\u001b[33m'\u001b[39m: importances\n\u001b[32m      7\u001b[39m })\n\u001b[32m      9\u001b[39m importance_df = importance_df.sort_values(by=\u001b[33m'\u001b[39m\u001b[33mImportance\u001b[39m\u001b[33m'\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "importances = model_test.feature_importance()\n",
    "feature_names = X.columns  # tomamos los nombres desde tu dataset original\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(importance_df.head(60))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4138f472",
   "metadata": {},
   "source": [
    "#Ganancia modelo clase 211 MARS sin nada\n",
    "Ganancia modelo optimizaod ac√°, 331 mars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a435b72",
   "metadata": {},
   "source": [
    "## Entrenar Modelos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c32f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo entrenado s√≥lo con 1 2 y 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa252171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo guardado...\n",
      "‚úÖ Modelo cargado exitosamente\n",
      "\n",
      "============================================================\n",
      "PREDICCIONES PARA KAGGLE\n",
      "============================================================\n",
      "Cargando n√∫mero √≥ptimo de env√≠os...\n",
      "N√∫mero √≥ptimo de env√≠os: 10,712\n",
      "\n",
      "Preparando datos de Kaggle...\n",
      "Datos de Kaggle: (164313, 802)\n",
      "Generando predicciones...\n",
      "Seleccionando top 10,712 clientes...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json \n",
    "# ============================================================\n",
    "# CARGAR MODELO GUARDADO\n",
    "# ============================================================\n",
    "\n",
    "print(\"Cargando modelo guardado...\")\n",
    "model_test = lgb.Booster(model_file='data/modelo_train_test.txt')\n",
    "print(\"‚úÖ Modelo cargado exitosamente\")\n",
    "\n",
    "# ============================================================\n",
    "# PREDECIR EN KAGGLE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREDICCIONES PARA KAGGLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cargar el n√∫mero √≥ptimo de env√≠os\n",
    "print(\"Cargando n√∫mero √≥ptimo de env√≠os...\")\n",
    "with open('data/resultados_test.json', 'r') as f:\n",
    "    resultados_test = json.load(f)\n",
    "\n",
    "n_envios_optimo = resultados_test['n_envios_optimo']\n",
    "print(f\"N√∫mero √≥ptimo de env√≠os: {n_envios_optimo:,}\")\n",
    "\n",
    "# Preparar datos de Kaggle\n",
    "print(\"\\nPreparando datos de Kaggle...\")\n",
    "X_kaggle = df_kaggle.drop(\"clase_ternaria\", axis=1).to_numpy().astype('float32')\n",
    "clientes_kaggle = df_kaggle[\"numero_de_cliente\"].to_numpy()\n",
    "\n",
    "print(f\"Datos de Kaggle: {X_kaggle.shape}\")\n",
    "\n",
    "# Predicciones probabil√≠sticas\n",
    "print(\"Generando predicciones...\")\n",
    "y_pred_prob = model_test.predict(X_kaggle)\n",
    "\n",
    "# Seleccionar top N clientes\n",
    "print(f\"Seleccionando top {n_envios_optimo:,} clientes...\")\n",
    "indices_top = np.argsort(-y_pred_prob)[:n_envios_optimo]\n",
    "\n",
    "# Crear predicci√≥n binaria\n",
    "y_pred_bin = np.zeros(len(y_pred_prob), dtype=int)\n",
    "y_pred_bin[indices_top] = 1\n",
    "\n",
    "# Crear submission\n",
    "submission = pd.DataFrame({\n",
    "    \"numero_de_cliente\": clientes_kaggle,\n",
    "    \"Predicted\": y_pred_bin\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eafd001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Archivo guardado: 'data/predicciones_kaggle.csv'\n",
      "Total registros: 164,313\n",
      "Predicciones positivas (1): 10,712\n",
      "Predicciones negativas (0): 153,601\n",
      "\n",
      "üìÑ Primeras filas:\n",
      "   numero_de_cliente  Predicted\n",
      "0          433987585          0\n",
      "1          434200463          0\n",
      "2          434285203          0\n",
      "3          434436672          0\n",
      "4          434468833          0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Guardar\n",
    "submission.to_csv(\"data/predicciones_kaggle_en_marz.csv\", index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Archivo guardado: 'data/predicciones_kaggle.csv'\")\n",
    "print(f\"Total registros: {len(submission):,}\")\n",
    "print(f\"Predicciones positivas (1): {y_pred_bin.sum():,}\")\n",
    "print(f\"Predicciones negativas (0): {(y_pred_bin == 0).sum():,}\")\n",
    "\n",
    "print(\"\\nüìÑ Primeras filas:\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7e2cad",
   "metadata": {},
   "source": [
    "## Modelo entrenado con 01..04"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8899a612",
   "metadata": {},
   "source": [
    "Modelo multiples semillas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e168b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PREPARANDO DATOS: TRAIN + TEST COMBINADOS\n",
      "============================================================\n",
      "Creando pesos en df_train...\n",
      "Creando pesos en df_test...\n",
      "Concatenando df_train y df_test...\n",
      "df_train: (197886, 802)\n",
      "df_test: (163418, 802)\n",
      "df_full: (361304, 802)\n",
      "\n",
      "Convirtiendo a numpy (float32)...\n",
      "X_full: (361304, 799), dtype: float32\n",
      "y_full_binaria: (361304,), dtype: int8\n",
      "Clase positiva: 7,369\n",
      "Clase negativa: 353,935\n",
      "\n",
      "Liberando df_train, df_test, df_full de memoria...\n",
      "‚úÖ Datos preparados\n",
      "\n",
      "============================================================\n",
      "CARGANDO HIPERPAR√ÅMETROS\n",
      "============================================================\n",
      "Trial 218\n",
      "Iteraciones: 98\n",
      "\n",
      "============================================================\n",
      "ENTRENANDO 5 MODELOS (UNO POR SEMILLA)\n",
      "============================================================\n",
      "\n",
      "--- Modelo 1/5 - Semilla: 550007 ---\n",
      "Entrenando con 98 iteraciones...\n",
      "‚úÖ Modelo guardado: 'data/predicciones/modelo_semilla_550007.txt'\n",
      "\n",
      "--- Modelo 2/5 - Semilla: 550019 ---\n",
      "Entrenando con 98 iteraciones...\n",
      "‚úÖ Modelo guardado: 'data/predicciones/modelo_semilla_550019.txt'\n",
      "\n",
      "--- Modelo 3/5 - Semilla: 550031 ---\n",
      "Entrenando con 98 iteraciones...\n",
      "‚úÖ Modelo guardado: 'data/predicciones/modelo_semilla_550031.txt'\n",
      "\n",
      "--- Modelo 4/5 - Semilla: 550033 ---\n",
      "Entrenando con 98 iteraciones...\n",
      "‚úÖ Modelo guardado: 'data/predicciones/modelo_semilla_550033.txt'\n",
      "\n",
      "--- Modelo 5/5 - Semilla: 550047 ---\n",
      "Entrenando con 98 iteraciones...\n",
      "‚úÖ Modelo guardado: 'data/predicciones/modelo_semilla_550047.txt'\n",
      "\n",
      "‚úÖ 5 modelos entrenados exitosamente\n",
      "\n",
      "============================================================\n",
      "CARGANDO DF_KAGGLE Y PREDICIENDO\n",
      "============================================================\n",
      "Leyendo df_kaggle...\n",
      "df_kaggle: (164313, 803)\n",
      "Tipo de numero_de_cliente: int64\n",
      "X_kaggle: (164313, 801), dtype=float32\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_full' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 195\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m df_kaggle\n\u001b[32m    192\u001b[39m gc.collect()\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m train_cols = \u001b[38;5;28mset\u001b[39m(\u001b[43mX_full\u001b[49m.columns)\n\u001b[32m    196\u001b[39m kaggle_cols = \u001b[38;5;28mset\u001b[39m(X_kaggle.columns)\n\u001b[32m    198\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExtra en kaggle:\u001b[39m\u001b[33m\"\u001b[39m, kaggle_cols - train_cols)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_full' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import json\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================================\n",
    "# PREPARAR DATOS: TRAIN + TEST COMBINADOS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PREPARANDO DATOS: TRAIN + TEST COMBINADOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Crear pesos en df_train\n",
    "print(\"Creando pesos en df_train...\")\n",
    "df_train['clase_peso'] = 1.0\n",
    "df_train.loc[df_train['clase_ternaria'] == 'BAJA+2', 'clase_peso'] = 1.00002\n",
    "df_train.loc[df_train['clase_ternaria'] == 'BAJA+1', 'clase_peso'] = 1.00001\n",
    "\n",
    "# # 2. Crear pesos en df_test\n",
    "print(\"Creando pesos en df_test...\")\n",
    "df_test['clase_peso'] = 1.0\n",
    "df_test.loc[df_test['clase_ternaria'] == 'BAJA+2', 'clase_peso'] = 1.00002\n",
    "df_test.loc[df_test['clase_ternaria'] == 'BAJA+1', 'clase_peso'] = 1.00001\n",
    "\n",
    "# # 3. Concatenar df_train y df_test\n",
    "print(\"Concatenando df_train y df_test...\")\n",
    "df_full = pd.concat([df_train, df_test], axis=0, ignore_index=True)\n",
    "\n",
    "print(f\"df_train: {df_train.shape}\")\n",
    "print(f\"df_test: {df_test.shape}\")\n",
    "print(f\"df_full: {df_full.shape}\")\n",
    "\n",
    "# 4. Convertir a numpy (optimizado) - SIN numero_de_cliente\n",
    "print(\"\\nConvirtiendo a numpy (float32)...\")\n",
    "X_full = df_full.drop([\"clase_ternaria\", \"clase_peso\", \"numero_de_cliente\"], axis=1).to_numpy().astype('float32')\n",
    "y_full = df_full[\"clase_ternaria\"].to_numpy()\n",
    "pesos_full = df_full[\"clase_peso\"].to_numpy().astype('float32')\n",
    "\n",
    "# Binarizar\n",
    "y_full_binaria = (y_full != \"CONTINUA\").astype('int8')\n",
    "\n",
    "print(f\"X_full: {X_full.shape}, dtype: {X_full.dtype}\")\n",
    "print(f\"y_full_binaria: {y_full_binaria.shape}, dtype: {y_full_binaria.dtype}\")\n",
    "print(f\"Clase positiva: {y_full_binaria.sum():,}\")\n",
    "print(f\"Clase negativa: {(y_full_binaria == 0).sum():,}\")\n",
    "\n",
    "# Liberar memoria\n",
    "print(\"\\nLiberando df_train, df_test, df_full de memoria...\")\n",
    "del df_train, df_test, df_full, y_full\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Datos preparados\")\n",
    "\n",
    "# ============================================================\n",
    "# DEFINIR M√âTRICA PERSONALIZADA DE GANANCIA\n",
    "# ============================================================\n",
    "\n",
    "ganancia_acierto = 780000\n",
    "costo_estimulo = 20000\n",
    "\n",
    "def ganancia_lgb(y_pred, data):\n",
    "    \"\"\"M√©trica personalizada de ganancia para LightGBM.\"\"\"\n",
    "    weight = data.get_weight()\n",
    "    \n",
    "    # Calcular ganancia para cada predicci√≥n\n",
    "    ganancia = np.where(weight == 1.00002, ganancia_acierto, 0) - costo_estimulo\n",
    "    \n",
    "    # Ordenar por predicci√≥n descendente\n",
    "    indices = np.argsort(-y_pred)\n",
    "    ganancia_ordenada = ganancia[indices]\n",
    "    \n",
    "    # Calcular ganancia acumulada m√°xima\n",
    "    ganancia_acum = np.cumsum(ganancia_ordenada)\n",
    "    max_ganancia = np.max(ganancia_acum)\n",
    "    \n",
    "    return 'ganancia', max_ganancia, True\n",
    "\n",
    "# ============================================================\n",
    "# CARGAR HIPERPAR√ÅMETROS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CARGANDO HIPERPAR√ÅMETROS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "with open('data/mejores_hiperparametros2.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "config = data[0] if isinstance(data, list) else data\n",
    "\n",
    "print(f\"Trial {config['trial_number']}\")\n",
    "print(f\"Iteraciones: {config['best_iter']}\")\n",
    "\n",
    "# ============================================================\n",
    "# ENTRENAR UN MODELO POR SEMILLA\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"ENTRENANDO {len(SEMILLAS)} MODELOS (UNO POR SEMILLA)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "modelos = []\n",
    "\n",
    "for i, semilla in enumerate(SEMILLAS):\n",
    "    print(f\"\\n--- Modelo {i+1}/{len(SEMILLAS)} - Semilla: {semilla} ---\")\n",
    "    \n",
    "    # Configurar par√°metros con la semilla correspondiente\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'custom',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'first_metric_only': True,\n",
    "        'boost_from_average': True,\n",
    "        'feature_pre_filter': False,\n",
    "        'max_bin': 31,\n",
    "        'num_leaves': config['params']['num_leaves'],\n",
    "        'learning_rate': config['params']['learning_rate'],\n",
    "        'min_data_in_leaf': config['params']['min_data_in_leaf'],\n",
    "        'feature_fraction': config['params']['feature_fraction'],\n",
    "        'bagging_fraction': config['params']['bagging_fraction'],\n",
    "        'seed': semilla,  # ‚úÖ Cambiar semilla\n",
    "        'verbose': -1  # Silencioso para no saturar output\n",
    "    }\n",
    "    \n",
    "    # Crear dataset\n",
    "    full_data = lgb.Dataset(X_full,\n",
    "                            label=y_full_binaria,\n",
    "                            weight=pesos_full)\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    print(f\"Entrenando con {config['best_iter']} iteraciones...\")\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        full_data,\n",
    "        num_boost_round=config['best_iter'],\n",
    "        feval=ganancia_lgb,\n",
    "        callbacks=[lgb.log_evaluation(period=0)]  # Sin logs\n",
    "    )\n",
    "    \n",
    "    # Guardar modelo\n",
    "    model.save_model(f'data/predicciones/modelo_semilla_{semilla}.txt')\n",
    "    print(f\"‚úÖ Modelo guardado: 'data/predicciones/modelo_semilla_{semilla}.txt'\")\n",
    "    \n",
    "    # Agregar a la lista\n",
    "    modelos.append(model)\n",
    "    \n",
    "    # Liberar memoria\n",
    "    del full_data\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n‚úÖ {len(modelos)} modelos entrenados exitosamente\")\n",
    "\n",
    "# Liberar X_full de memoria\n",
    "del X_full, y_full_binaria, pesos_full\n",
    "gc.collect()\n",
    "\n",
    "# ============================================================\n",
    "# CARGAR DF_KAGGLE Y PREDECIR CON CADA MODELO\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CARGANDO DF_KAGGLE Y PREDICIENDO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Leer df_kaggle con tipos optimizados\n",
    "print(\"Leyendo df_kaggle...\")\n",
    "columnas = pd.read_csv(\"data/df_kaggle.csv\", nrows=0).columns.tolist()\n",
    "\n",
    "# Crear diccionario de tipos: float32 para todas EXCEPTO numero_de_cliente\n",
    "dtypes = {col: 'float32' for col in columnas if col not in ['numero_de_cliente', 'clase_ternaria']}\n",
    "\n",
    "df_kaggle = pd.read_csv(\n",
    "    \"data/df_kaggle.csv\",\n",
    "    dtype=dtypes,\n",
    "    low_memory=True\n",
    ")\n",
    "\n",
    "print(f\"df_kaggle: {df_kaggle.shape}\")\n",
    "print(f\"Tipo de numero_de_cliente: {df_kaggle['numero_de_cliente'].dtype}\")\n",
    "\n",
    "\n",
    "# Preparar datos\n",
    "clientes_kaggle = df_kaggle[\"numero_de_cliente\"].values\n",
    "X_kaggle = df_kaggle.drop([\"clase_ternaria\", \"numero_de_cliente\"], axis=1).to_numpy().astype('float32')\n",
    "\n",
    "print(f\"X_kaggle: {X_kaggle.shape}, dtype={X_kaggle.dtype}\")\n",
    "\n",
    "del df_kaggle\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11d8c05a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_kaggle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_kaggle = \u001b[43mdf_kaggle\u001b[49m.drop(columns=[\u001b[33m'\u001b[39m\u001b[33mcprestamos_personales\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmprestamos_personales\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'df_kaggle' is not defined"
     ]
    }
   ],
   "source": [
    "df_kaggle = df_kaggle.drop(columns=['cprestamos_personales', 'mprestamos_personales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e69820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# PREDECIR CON CADA MODELO Y PROMEDIAR\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERANDO PREDICCIONES CON CADA MODELO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Inicializar array para acumular probabilidades\n",
    "probabilidades_acumuladas = np.zeros(len(X_kaggle), dtype='float64')\n",
    "\n",
    "for i, (model, semilla) in enumerate(zip(modelos, SEMILLAS)):\n",
    "    print(f\"\\nModelo {i+1}/{len(modelos)} - Semilla {semilla}\")\n",
    "    print(\"Prediciendo...\")\n",
    "    \n",
    "    y_pred_prob = model.predict(X_kaggle)\n",
    "    \n",
    "    print(f\"  Prob min: {y_pred_prob.min():.6f}\")\n",
    "    print(f\"  Prob max: {y_pred_prob.max():.6f}\")\n",
    "    print(f\"  Prob media: {y_pred_prob.mean():.6f}\")\n",
    "    \n",
    "    # Acumular probabilidades\n",
    "    probabilidades_acumuladas += y_pred_prob\n",
    "    \n",
    "    # Liberar modelo de memoria\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "# Promediar probabilidades\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROMEDIANDO PROBABILIDADES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_pred_prob_promedio = probabilidades_acumuladas / len(SEMILLAS)\n",
    "\n",
    "print(f\"Probabilidades promediadas:\")\n",
    "print(f\"  Prob min: {y_pred_prob_promedio.min():.6f}\")\n",
    "print(f\"  Prob max: {y_pred_prob_promedio.max():.6f}\")\n",
    "print(f\"  Prob media: {y_pred_prob_promedio.mean():.6f}\")\n",
    "\n",
    "# Guardar probabilidades promediadas (opcional)\n",
    "np.save('data/predicciones/probabilidades_promedio.npy', y_pred_prob_promedio)\n",
    "print(\"\\n‚úÖ Probabilidades guardadas en 'data/predicciones/probabilidades_promedio.npy'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "504cf595",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_full' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_cols = \u001b[38;5;28mset\u001b[39m(\u001b[43mX_full\u001b[49m.columns)\n\u001b[32m      2\u001b[39m kaggle_cols = \u001b[38;5;28mset\u001b[39m(X_kaggle.columns)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExtra en kaggle:\u001b[39m\u001b[33m\"\u001b[39m, kaggle_cols - train_cols)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_full' is not defined"
     ]
    }
   ],
   "source": [
    "train_cols = set(X_full.columns)\n",
    "kaggle_cols = set(X_kaggle.columns)\n",
    "\n",
    "print(\"Extra en kaggle:\", kaggle_cols - train_cols)\n",
    "print(\"Faltan en kaggle:\", train_cols - kaggle_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a95e552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BINARIZANDO PREDICCIONES\n",
      "============================================================\n",
      "Threshold de probabilidad: 0.03\n",
      "Predicciones positivas (prob > 0.03): 14,668\n",
      "% del total: 8.93%\n",
      "\n",
      "============================================================\n",
      "CREANDO SUBMISSION\n",
      "============================================================\n",
      "Tipos en submission:\n",
      "  numero_de_cliente: int64\n",
      "  Predicted: int8\n",
      "\n",
      "‚úÖ Archivo guardado: 'data/predicciones_kaggle_ensemble_5semillas_threshold0.03_20251031_1732.csv'\n",
      "\n",
      "üìä RESUMEN:\n",
      "  Modelos entrenados: 5\n",
      "  Total registros: 164,313\n",
      "  Predicciones positivas (1): 14,668\n",
      "  Predicciones negativas (0): 149,645\n",
      "  % positivos: 8.93%\n",
      "\n",
      "üìÑ Primeras 10 filas:\n",
      "   numero_de_cliente  Predicted\n",
      "0          433987585          0\n",
      "1          434200463          1\n",
      "2          434285203          0\n",
      "3          434436672          0\n",
      "4          434468833          1\n",
      "5          434500404          0\n",
      "6          434626792          0\n",
      "7          434672369          0\n",
      "8          434687467          0\n",
      "9          434694905          0\n",
      "\n",
      "Verificaci√≥n de IDs:\n",
      "  Primer ID: 433987585\n",
      "  √öltimo ID: 951048415\n",
      "\n",
      "üéâ Ensemble de modelos completado - Predicciones listas para Kaggle!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# BINARIZAR CON THRESHOLD DE PROBABILIDAD\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BINARIZANDO PREDICCIONES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "threshold = 0.030\n",
    "print(f\"Threshold de probabilidad: {threshold}\")\n",
    "\n",
    "# Binarizar seg√∫n threshold\n",
    "y_pred_bin = (y_pred_prob_promedio > threshold).astype('int8')\n",
    "\n",
    "print(f\"Predicciones positivas (prob > {threshold}): {y_pred_bin.sum():,}\")\n",
    "print(f\"% del total: {y_pred_bin.sum() / len(y_pred_bin) * 100:.2f}%\")\n",
    "\n",
    "# ============================================================\n",
    "# CREAR SUBMISSION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREANDO SUBMISSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"numero_de_cliente\": clientes_kaggle,\n",
    "    \"Predicted\": y_pred_bin\n",
    "})\n",
    "\n",
    "# Verificar tipos\n",
    "print(f\"Tipos en submission:\")\n",
    "print(f\"  numero_de_cliente: {submission['numero_de_cliente'].dtype}\")\n",
    "print(f\"  Predicted: {submission['Predicted'].dtype}\")\n",
    "\n",
    "# Obtener fecha y hora actual en formato YYYYMMDD_HHMM\n",
    "fecha = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "# Crear el nombre del archivo con fecha incluida\n",
    "filename = f\"data/predicciones_kaggle_ensemble_{len(SEMILLAS)}semillas_threshold{threshold}_{fecha}.csv\"\n",
    "submission.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Archivo guardado: '{filename}'\")\n",
    "print(f\"\\nüìä RESUMEN:\")\n",
    "print(f\"  Modelos entrenados: {len(SEMILLAS)}\")\n",
    "print(f\"  Total registros: {len(submission):,}\")\n",
    "print(f\"  Predicciones positivas (1): {y_pred_bin.sum():,}\")\n",
    "print(f\"  Predicciones negativas (0): {(y_pred_bin == 0).sum():,}\")\n",
    "print(f\"  % positivos: {y_pred_bin.sum() / len(submission) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nüìÑ Primeras 10 filas:\")\n",
    "print(submission.head(10))\n",
    "\n",
    "print(\"\\nVerificaci√≥n de IDs:\")\n",
    "print(f\"  Primer ID: {submission['numero_de_cliente'].iloc[0]}\")\n",
    "print(f\"  √öltimo ID: {submission['numero_de_cliente'].iloc[-1]}\")\n",
    "\n",
    "print(\"\\nüéâ Ensemble de modelos completado - Predicciones listas para Kaggle!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6831d497",
   "metadata": {},
   "source": [
    "## -4k era entrenado con 01..03 y predecir 06 con los mejores hiperparametros de mi primer corrida (30) con metric ganancia\n",
    "## -3k simil anterior pero modelo reentrenado con 01..04 con metric AUC \n",
    "## -3 K simil anterior pero modelo reentrenado con 01..04 con metric custom\n",
    "## -2 umbral fijo de 12k \n",
    "## 0 de 01 02 y 03 s√≥lo los baja+1/2. \n",
    "\n",
    "## voy a probar con slopes. no me sirvi√≥. solo con dl tampoco (aunque no optimic√© hip para esto)\n",
    "\n",
    "## 2 con ensamble de semillas\n",
    "1 si dejo los continua de marzo\n",
    "4 con hiperparametros optimizados 500. hip2\n",
    "5 con num fijo de 12300. esto es con fe todo sin slopes\n",
    "3 Optimizacion con Claude y regularizacion. NO FUNCIONO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647da6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "============================================================\n",
      "INICIANDO ENTRENAMIENTO CON DATASETS SLOPE\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "CARGANDO DATASETS SLOPE\n",
      "============================================================\n",
      "df_train_slope: (486791, 803)\n",
      "df_test_slope: (163418, 803)\n",
      "df_kaggle_slope: (164313, 803)\n",
      "\n",
      "============================================================\n",
      "PREPARANDO DATOS SLOPE: TRAIN + TEST COMBINADOS\n",
      "============================================================\n",
      "Creando pesos en df_train_slope...\n",
      "Creando pesos en df_test_slope...\n",
      "Concatenando df_train_slope y df_test_slope...\n",
      "df_train_slope: (486791, 804)\n",
      "df_test_slope: (163418, 804)\n",
      "df_full_slope: (650209, 804)\n",
      "\n",
      "Convirtiendo a numpy (float32)...\n",
      "X_full_slope: (650209, 801), dtype: float32\n",
      "y_full_slope_binaria: (650209,), dtype: int8\n",
      "Clase positiva: 7,369\n",
      "Clase negativa: 642,840\n",
      "\n",
      "Liberando df_train_slope, df_test_slope, df_full_slope de memoria...\n",
      "‚úÖ Datos SLOPE preparados\n",
      "\n",
      "============================================================\n",
      "ENTRENANDO 5 MODELOS SLOPE (UNO POR SEMILLA)\n",
      "============================================================\n",
      "\n",
      "--- Modelo SLOPE 1/5 - Semilla: 550007 ---\n",
      "Entrenando con 98 iteraciones...\n",
      "‚úÖ Modelo SLOPE guardado: 'data/predicciones/modelo_slope_semilla_550007.txt'\n",
      "\n",
      "--- Modelo SLOPE 2/5 - Semilla: 550019 ---\n",
      "Entrenando con 98 iteraciones...\n",
      "‚úÖ Modelo SLOPE guardado: 'data/predicciones/modelo_slope_semilla_550019.txt'\n",
      "\n",
      "--- Modelo SLOPE 3/5 - Semilla: 550031 ---\n",
      "Entrenando con 98 iteraciones...\n",
      "‚úÖ Modelo SLOPE guardado: 'data/predicciones/modelo_slope_semilla_550031.txt'\n",
      "\n",
      "--- Modelo SLOPE 4/5 - Semilla: 550033 ---\n",
      "Entrenando con 98 iteraciones...\n",
      "‚úÖ Modelo SLOPE guardado: 'data/predicciones/modelo_slope_semilla_550033.txt'\n",
      "\n",
      "--- Modelo SLOPE 5/5 - Semilla: 550047 ---\n",
      "Entrenando con 98 iteraciones...\n",
      "‚úÖ Modelo SLOPE guardado: 'data/predicciones/modelo_slope_semilla_550047.txt'\n",
      "\n",
      "‚úÖ 5 modelos SLOPE entrenados exitosamente\n",
      "\n",
      "============================================================\n",
      "CARGANDO DF_KAGGLE_SLOPE Y PREDICIENDO\n",
      "============================================================\n",
      "Leyendo df_kaggle_slope...\n",
      "df_kaggle_slope: (164313, 803)\n",
      "Tipo de numero_de_cliente: int64\n",
      "X_kaggle_slope: (164313, 801), dtype=float32\n",
      "\n",
      "============================================================\n",
      "GENERANDO PREDICCIONES SLOPE CON CADA MODELO\n",
      "============================================================\n",
      "\n",
      "Modelo SLOPE 1/5 - Semilla 550007\n",
      "Prediciendo...\n",
      "  Prob min: 0.000026\n",
      "  Prob max: 0.999898\n",
      "  Prob media: 0.014825\n",
      "\n",
      "Modelo SLOPE 2/5 - Semilla 550019\n",
      "Prediciendo...\n",
      "  Prob min: 0.000030\n",
      "  Prob max: 0.999576\n",
      "  Prob media: 0.015530\n",
      "\n",
      "Modelo SLOPE 3/5 - Semilla 550031\n",
      "Prediciendo...\n",
      "  Prob min: 0.000026\n",
      "  Prob max: 0.999677\n",
      "  Prob media: 0.015233\n",
      "\n",
      "Modelo SLOPE 4/5 - Semilla 550033\n",
      "Prediciendo...\n",
      "  Prob min: 0.000030\n",
      "  Prob max: 0.999720\n",
      "  Prob media: 0.015308\n",
      "\n",
      "Modelo SLOPE 5/5 - Semilla 550047\n",
      "Prediciendo...\n",
      "  Prob min: 0.000027\n",
      "  Prob max: 0.999092\n",
      "  Prob media: 0.015824\n",
      "\n",
      "============================================================\n",
      "PROMEDIANDO PROBABILIDADES SLOPE\n",
      "============================================================\n",
      "Probabilidades promediadas SLOPE:\n",
      "  Prob min: 0.000036\n",
      "  Prob max: 0.999177\n",
      "  Prob media: 0.015344\n",
      "\n",
      "‚úÖ Probabilidades SLOPE guardadas en 'data/predicciones/probabilidades_promedio_slope.npy'\n",
      "\n",
      "============================================================\n",
      "BINARIZANDO PREDICCIONES SLOPE\n",
      "============================================================\n",
      "Threshold de probabilidad SLOPE: 0.025\n",
      "Predicciones positivas (prob > 0.025): 14,251\n",
      "% del total: 8.67%\n",
      "\n",
      "============================================================\n",
      "CREANDO SUBMISSION SLOPE\n",
      "============================================================\n",
      "Tipos en submission SLOPE:\n",
      "  numero_de_cliente: int64\n",
      "  Predicted: int8\n",
      "\n",
      "‚úÖ Archivo SLOPE guardado: 'data/predicciones_kaggle_SLOPE_ensemble_5semillas_threshold0.025_20251022_1609.csv'\n",
      "\n",
      "üìä RESUMEN SLOPE:\n",
      "  Modelos entrenados: 5\n",
      "  Total registros: 164,313\n",
      "  Predicciones positivas (1): 14,251\n",
      "  Predicciones negativas (0): 150,062\n",
      "  % positivos: 8.67%\n",
      "\n",
      "üìÑ Primeras 10 filas SLOPE:\n",
      "   numero_de_cliente  Predicted\n",
      "0          433987585          0\n",
      "1          434200463          1\n",
      "2          434285203          0\n",
      "3          434436672          0\n",
      "4          434468833          1\n",
      "5          434500404          0\n",
      "6          434626792          0\n",
      "7          434672369          0\n",
      "8          434687467          0\n",
      "9          434694905          0\n",
      "\n",
      "Verificaci√≥n de IDs SLOPE:\n",
      "  Primer ID: 433987585\n",
      "  √öltimo ID: 951048415\n",
      "\n",
      "üéâ Ensemble de modelos SLOPE completado - Predicciones listas para Kaggle!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PARTE 2: ENTRENAMIENTO CON DATASETS con todos los continua\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=\"*60)\n",
    "print(\"INICIANDO ENTRENAMIENTO CON DATASETS SLOPE\")\n",
    "print(\"=\"*60)\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================\n",
    "# CARGAR DATASETS SLOPE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CARGANDO DATASETS SLOPE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_train_slope = pd.read_csv(\"data/df_train_01_02_03.csv\")\n",
    "df_test_slope = pd.read_csv(\"data/df_test_04.csv\")\n",
    "df_kaggle_slope = pd.read_csv(\"data/df_kaggle.csv\")\n",
    "\n",
    "print(f\"df_train_slope: {df_train_slope.shape}\")\n",
    "print(f\"df_test_slope: {df_test_slope.shape}\")\n",
    "print(f\"df_kaggle_slope: {df_kaggle_slope.shape}\")\n",
    "\n",
    "# ============================================================\n",
    "# PREPARAR DATOS SLOPE: TRAIN + TEST COMBINADOS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPARANDO DATOS SLOPE: TRAIN + TEST COMBINADOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Crear pesos en df_train_slope\n",
    "print(\"Creando pesos en df_train_slope...\")\n",
    "df_train_slope['clase_peso'] = 1.0\n",
    "df_train_slope.loc[df_train_slope['clase_ternaria'] == 'BAJA+2', 'clase_peso'] = 1.00002\n",
    "df_train_slope.loc[df_train_slope['clase_ternaria'] == 'BAJA+1', 'clase_peso'] = 1.00001\n",
    "\n",
    "# 2. Crear pesos en df_test_slope\n",
    "print(\"Creando pesos en df_test_slope...\")\n",
    "df_test_slope['clase_peso'] = 1.0\n",
    "df_test_slope.loc[df_test_slope['clase_ternaria'] == 'BAJA+2', 'clase_peso'] = 1.00002\n",
    "df_test_slope.loc[df_test_slope['clase_ternaria'] == 'BAJA+1', 'clase_peso'] = 1.00001\n",
    "\n",
    "# 3. Concatenar df_train_slope y df_test_slope\n",
    "print(\"Concatenando df_train_slope y df_test_slope...\")\n",
    "df_full_slope = pd.concat([df_train_slope, df_test_slope], axis=0, ignore_index=True)\n",
    "\n",
    "print(f\"df_train_slope: {df_train_slope.shape}\")\n",
    "print(f\"df_test_slope: {df_test_slope.shape}\")\n",
    "print(f\"df_full_slope: {df_full_slope.shape}\")\n",
    "\n",
    "# 4. Convertir a numpy (optimizado) - SIN numero_de_cliente\n",
    "print(\"\\nConvirtiendo a numpy (float32)...\")\n",
    "X_full_slope = df_full_slope.drop([\"clase_ternaria\", \"clase_peso\", \"numero_de_cliente\"], axis=1).to_numpy().astype('float32')\n",
    "y_full_slope = df_full_slope[\"clase_ternaria\"].to_numpy()\n",
    "pesos_full_slope = df_full_slope[\"clase_peso\"].to_numpy().astype('float32')\n",
    "\n",
    "# Binarizar\n",
    "y_full_slope_binaria = (y_full_slope != \"CONTINUA\").astype('int8')\n",
    "\n",
    "print(f\"X_full_slope: {X_full_slope.shape}, dtype: {X_full_slope.dtype}\")\n",
    "print(f\"y_full_slope_binaria: {y_full_slope_binaria.shape}, dtype: {y_full_slope_binaria.dtype}\")\n",
    "print(f\"Clase positiva: {y_full_slope_binaria.sum():,}\")\n",
    "print(f\"Clase negativa: {(y_full_slope_binaria == 0).sum():,}\")\n",
    "\n",
    "# Liberar memoria\n",
    "print(\"\\nLiberando df_train_slope, df_test_slope, df_full_slope de memoria...\")\n",
    "del df_train_slope, df_test_slope, df_full_slope, y_full_slope\n",
    "gc.collect()\n",
    "\n",
    "print(\"‚úÖ Datos SLOPE preparados\")\n",
    "\n",
    "# ============================================================\n",
    "# ENTRENAR MODELOS SLOPE (UNO POR SEMILLA)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"ENTRENANDO {len(SEMILLAS)} MODELOS SLOPE (UNO POR SEMILLA)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "modelos_slope = []\n",
    "\n",
    "for i, semilla in enumerate(SEMILLAS):\n",
    "    print(f\"\\n--- Modelo SLOPE {i+1}/{len(SEMILLAS)} - Semilla: {semilla} ---\")\n",
    "    \n",
    "    # Configurar par√°metros con la semilla correspondiente (mismos hiperpar√°metros)\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'custom',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'first_metric_only': True,\n",
    "        'boost_from_average': True,\n",
    "        'feature_pre_filter': False,\n",
    "        'max_bin': 31,\n",
    "        'num_leaves': config['params']['num_leaves'],\n",
    "        'learning_rate': config['params']['learning_rate'],\n",
    "        'min_data_in_leaf': config['params']['min_data_in_leaf'],\n",
    "        'feature_fraction': config['params']['feature_fraction'],\n",
    "        'bagging_fraction': config['params']['bagging_fraction'],\n",
    "        'seed': semilla,\n",
    "        'verbose': -1, \n",
    "        'is_unbalanced': True  # Manejar desbalanceo autom√°ticamente\n",
    "    }\n",
    "    \n",
    "    # Crear dataset\n",
    "    full_data_slope = lgb.Dataset(X_full_slope,\n",
    "                                   label=y_full_slope_binaria,\n",
    "                                   weight=pesos_full_slope)\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    print(f\"Entrenando con {config['best_iter']} iteraciones...\")\n",
    "    model_slope = lgb.train(\n",
    "        params,\n",
    "        full_data_slope,\n",
    "        num_boost_round=config['best_iter'],\n",
    "        feval=ganancia_lgb,\n",
    "        callbacks=[lgb.log_evaluation(period=0)]\n",
    "    )\n",
    "    \n",
    "    # Guardar modelo\n",
    "    model_slope.save_model(f'data/predicciones/modelo_slope_semilla_{semilla}.txt')\n",
    "    print(f\"‚úÖ Modelo SLOPE guardado: 'data/predicciones/modelo_slope_semilla_{semilla}.txt'\")\n",
    "    \n",
    "    # Agregar a la lista\n",
    "    modelos_slope.append(model_slope)\n",
    "    \n",
    "    # Liberar memoria\n",
    "    del full_data_slope\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n‚úÖ {len(modelos_slope)} modelos SLOPE entrenados exitosamente\")\n",
    "\n",
    "# Liberar X_full_slope de memoria\n",
    "del X_full_slope, y_full_slope_binaria, pesos_full_slope\n",
    "gc.collect()\n",
    "\n",
    "# ============================================================\n",
    "# CARGAR DF_KAGGLE_SLOPE Y PREDECIR\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CARGANDO DF_KAGGLE_SLOPE Y PREDICIENDO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Leer df_kaggle_slope con tipos optimizados\n",
    "print(\"Leyendo df_kaggle_slope...\")\n",
    "columnas_slope = pd.read_csv(\"data/df_kaggle.csv\", nrows=0).columns.tolist()\n",
    "\n",
    "# Crear diccionario de tipos: float32 para todas EXCEPTO numero_de_cliente\n",
    "dtypes_slope = {col: 'float32' for col in columnas_slope if col not in ['numero_de_cliente', 'clase_ternaria']}\n",
    "\n",
    "df_kaggle_slope = pd.read_csv(\n",
    "    \"data/df_kaggle.csv\",\n",
    "    dtype=dtypes_slope,\n",
    "    low_memory=True\n",
    ")\n",
    "\n",
    "print(f\"df_kaggle_slope: {df_kaggle_slope.shape}\")\n",
    "print(f\"Tipo de numero_de_cliente: {df_kaggle_slope['numero_de_cliente'].dtype}\")\n",
    "\n",
    "# Preparar datos\n",
    "clientes_kaggle_slope = df_kaggle_slope[\"numero_de_cliente\"].values\n",
    "X_kaggle_slope = df_kaggle_slope.drop([\"clase_ternaria\", \"numero_de_cliente\"], axis=1).to_numpy().astype('float32')\n",
    "\n",
    "print(f\"X_kaggle_slope: {X_kaggle_slope.shape}, dtype={X_kaggle_slope.dtype}\")\n",
    "\n",
    "del df_kaggle_slope\n",
    "gc.collect()\n",
    "\n",
    "# ============================================================\n",
    "# PREDECIR CON CADA MODELO SLOPE Y PROMEDIAR\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERANDO PREDICCIONES SLOPE CON CADA MODELO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Inicializar array para acumular probabilidades\n",
    "probabilidades_acumuladas_slope = np.zeros(len(X_kaggle_slope), dtype='float64')\n",
    "\n",
    "for i, (model_slope, semilla) in enumerate(zip(modelos_slope, SEMILLAS)):\n",
    "    print(f\"\\nModelo SLOPE {i+1}/{len(modelos_slope)} - Semilla {semilla}\")\n",
    "    print(\"Prediciendo...\")\n",
    "    \n",
    "    y_pred_prob_slope = model_slope.predict(X_kaggle_slope)\n",
    "    \n",
    "    print(f\"  Prob min: {y_pred_prob_slope.min():.6f}\")\n",
    "    print(f\"  Prob max: {y_pred_prob_slope.max():.6f}\")\n",
    "    print(f\"  Prob media: {y_pred_prob_slope.mean():.6f}\")\n",
    "    \n",
    "    # Acumular probabilidades\n",
    "    probabilidades_acumuladas_slope += y_pred_prob_slope\n",
    "    \n",
    "    # Liberar modelo de memoria\n",
    "    del model_slope\n",
    "    gc.collect()\n",
    "\n",
    "# Promediar probabilidades\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROMEDIANDO PROBABILIDADES SLOPE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_pred_prob_promedio_slope = probabilidades_acumuladas_slope / len(SEMILLAS)\n",
    "\n",
    "print(f\"Probabilidades promediadas SLOPE:\")\n",
    "print(f\"  Prob min: {y_pred_prob_promedio_slope.min():.6f}\")\n",
    "print(f\"  Prob max: {y_pred_prob_promedio_slope.max():.6f}\")\n",
    "print(f\"  Prob media: {y_pred_prob_promedio_slope.mean():.6f}\")\n",
    "\n",
    "# Guardar probabilidades promediadas\n",
    "np.save('data/predicciones/probabilidades_promedio_slope.npy', y_pred_prob_promedio_slope)\n",
    "print(\"\\n‚úÖ Probabilidades SLOPE guardadas en 'data/predicciones/probabilidades_promedio_slope.npy'\")\n",
    "\n",
    "# ============================================================\n",
    "# BINARIZAR CON THRESHOLD DE PROBABILIDAD SLOPE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BINARIZANDO PREDICCIONES SLOPE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "threshold_slope = 0.025\n",
    "print(f\"Threshold de probabilidad SLOPE: {threshold_slope}\")\n",
    "\n",
    "# Binarizar seg√∫n threshold\n",
    "y_pred_bin_slope = (y_pred_prob_promedio_slope > threshold_slope).astype('int8')\n",
    "\n",
    "print(f\"Predicciones positivas (prob > {threshold_slope}): {y_pred_bin_slope.sum():,}\")\n",
    "print(f\"% del total: {y_pred_bin_slope.sum() / len(y_pred_bin_slope) * 100:.2f}%\")\n",
    "\n",
    "# ============================================================\n",
    "# CREAR SUBMISSION SLOPE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREANDO SUBMISSION SLOPE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "submission_slope = pd.DataFrame({\n",
    "    \"numero_de_cliente\": clientes_kaggle_slope,\n",
    "    \"Predicted\": y_pred_bin_slope\n",
    "})\n",
    "\n",
    "# Verificar tipos\n",
    "print(f\"Tipos en submission SLOPE:\")\n",
    "print(f\"  numero_de_cliente: {submission_slope['numero_de_cliente'].dtype}\")\n",
    "print(f\"  Predicted: {submission_slope['Predicted'].dtype}\")\n",
    "\n",
    "# Obtener fecha y hora actual\n",
    "fecha_slope = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "# Crear el nombre del archivo con fecha incluida\n",
    "filename_slope = f\"data/predicciones_kaggle_SLOPE_ensemble_{len(SEMILLAS)}semillas_threshold{threshold_slope}_{fecha_slope}.csv\"\n",
    "submission_slope.to_csv(filename_slope, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Archivo SLOPE guardado: '{filename_slope}'\")\n",
    "print(f\"\\nüìä RESUMEN SLOPE:\")\n",
    "print(f\"  Modelos entrenados: {len(SEMILLAS)}\")\n",
    "print(f\"  Total registros: {len(submission_slope):,}\")\n",
    "print(f\"  Predicciones positivas (1): {y_pred_bin_slope.sum():,}\")\n",
    "print(f\"  Predicciones negativas (0): {(y_pred_bin_slope == 0).sum():,}\")\n",
    "print(f\"  % positivos: {y_pred_bin_slope.sum() / len(submission_slope) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nüìÑ Primeras 10 filas SLOPE:\")\n",
    "print(submission_slope.head(10))\n",
    "\n",
    "print(\"\\nVerificaci√≥n de IDs SLOPE:\")\n",
    "print(f\"  Primer ID: {submission_slope['numero_de_cliente'].iloc[0]}\")\n",
    "print(f\"  √öltimo ID: {submission_slope['numero_de_cliente'].iloc[-1]}\")\n",
    "\n",
    "print(\"\\nüéâ Ensemble de modelos SLOPE completado - Predicciones listas para Kaggle!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b104e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "============================================================\n",
      "ENSAMBLE DE PREDICCIONES: EST√ÅNDAR + SLOPE\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ALINEANDO DATASETS POR NUMERO_DE_CLIENTE\n",
      "============================================================\n",
      "Registros modelo est√°ndar: 164,313\n",
      "Registros modelo SLOPE: 164,313\n",
      "\n",
      "‚úÖ Registros despu√©s del merge: 164,313\n",
      "\n",
      "============================================================\n",
      "M√âTODO 1: ENSAMBLE POR VOTACI√ìN (UNI√ìN)\n",
      "============================================================\n",
      "\n",
      "üìä ESTAD√çSTICAS DE VOTACI√ìN:\n",
      "  Predicciones modelo est√°ndar (1): 14,002\n",
      "  Predicciones modelo SLOPE (1): 14,251\n",
      "  Predicciones UNI√ìN (1): 17,569\n",
      "  % del total: 10.69%\n",
      "\n",
      "üîç AN√ÅLISIS DE OVERLAP:\n",
      "  Solo modelo est√°ndar: 3,318\n",
      "  Solo modelo SLOPE: 3,567\n",
      "  Ambos modelos: 10,684\n",
      "  Jaccard similarity: 60.81%\n",
      "\n",
      "‚úÖ Archivo votaci√≥n guardado: 'data/predicciones_kaggle_ENSEMBLE_VOTACION_5semillas_20251022_1611.csv'\n",
      "\n",
      "============================================================\n",
      "M√âTODO 2: ENSAMBLE POR PROMEDIO DE PROBABILIDADES\n",
      "============================================================\n",
      "Probabilidades ensemble:\n",
      "  Prob min: 0.000059\n",
      "  Prob max: 0.953811\n",
      "  Prob media: 0.014151\n",
      "\n",
      "‚úÖ Probabilidades ensemble guardadas\n",
      "\n",
      "--- Opci√≥n 2A: Usar threshold en ensemble ---\n",
      "Threshold ensemble: 0.025\n",
      "Predicciones positivas: 14,725\n",
      "% del total: 8.96%\n",
      "‚úÖ Archivo ensemble guardado: 'data/predicciones_kaggle_ENSEMBLE_PROMEDIO_5semillas_threshold0.025_20251022_1611.csv'\n",
      "\n",
      "--- Opci√≥n 2B: Generar m√∫ltiples versiones con diferentes thresholds ---\n",
      "\n",
      "Generando ensemble con threshold 0.015...\n",
      "  ‚úÖ Guardado: data/predicciones/ensemble_promedio_threshold0.015.csv (20,996 positivos - 12.78%)\n",
      "\n",
      "Generando ensemble con threshold 0.02...\n",
      "  ‚úÖ Guardado: data/predicciones/ensemble_promedio_threshold0.02.csv (17,219 positivos - 10.48%)\n",
      "\n",
      "Generando ensemble con threshold 0.025...\n",
      "  ‚úÖ Guardado: data/predicciones/ensemble_promedio_threshold0.025.csv (14,725 positivos - 8.96%)\n",
      "\n",
      "Generando ensemble con threshold 0.03...\n",
      "  ‚úÖ Guardado: data/predicciones/ensemble_promedio_threshold0.03.csv (12,899 positivos - 7.85%)\n",
      "\n",
      "Generando ensemble con threshold 0.035...\n",
      "  ‚úÖ Guardado: data/predicciones/ensemble_promedio_threshold0.035.csv (11,504 positivos - 7.00%)\n",
      "\n",
      "Generando ensemble con threshold 0.04...\n",
      "  ‚úÖ Guardado: data/predicciones/ensemble_promedio_threshold0.04.csv (10,385 positivos - 6.32%)\n",
      "\n",
      "============================================================\n",
      "RESUMEN COMPARATIVO DE ENSAMBLES\n",
      "============================================================\n",
      "\n",
      "\n",
      "                   M√©todo  N¬∞ Env√≠os % Base\n",
      "          Modelo Est√°ndar      14002  8.52%\n",
      "             Modelo SLOPE      14251  8.67%\n",
      "Ensemble Votaci√≥n (Uni√≥n)      17569 10.69%\n",
      "        Ensemble Promedio      14725  8.96%\n",
      "\n",
      "============================================================\n",
      "AN√ÅLISIS DE CORRELACI√ìN\n",
      "============================================================\n",
      "Correlaci√≥n entre probabilidades (Est√°ndar vs SLOPE): 0.7742\n",
      "  üîÑ Correlaci√≥n moderada - buena diversidad para ensemble\n",
      "\n",
      "============================================================\n",
      "AN√ÅLISIS DE PREDICCIONES DIVERGENTES\n",
      "============================================================\n",
      "\n",
      "Top 10 clientes con mayor divergencia en probabilidades:\n",
      "Cliente              Prob Est√°ndar   Prob SLOPE      Diferencia     \n",
      "-----------------------------------------------------------------\n",
      "269126547.0          0.012054        0.997068        0.985014       \n",
      "933938998.0          0.003951        0.985185        0.981235       \n",
      "1177780263.0         0.008928        0.988145        0.979217       \n",
      "987766317.0          0.012198        0.991297        0.979099       \n",
      "1006795253.0         0.009837        0.986226        0.976389       \n",
      "1079026665.0         0.019023        0.992933        0.973910       \n",
      "1222235148.0         0.029376        0.997805        0.968429       \n",
      "1050031151.0         0.009470        0.977270        0.967800       \n",
      "828693938.0          0.019072        0.986473        0.967401       \n",
      "1045049505.0         0.011463        0.976270        0.964807       \n",
      "\n",
      "============================================================\n",
      "GUARDANDO AN√ÅLISIS COMPLETO\n",
      "============================================================\n",
      "‚úÖ An√°lisis guardado en 'data/analisis_ensemble.json'\n",
      "\n",
      "============================================================\n",
      "============================================================\n",
      "üéâ PROCESO DE ENSEMBLE COMPLETADO\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "üìÅ ARCHIVOS GENERADOS:\n",
      "  1Ô∏è‚É£  Modelo Est√°ndar: data/predicciones_kaggle_ensemble_5semillas_threshold0.025_20251022_1552.csv\n",
      "  2Ô∏è‚É£  Modelo SLOPE: data/predicciones_kaggle_SLOPE_ensemble_5semillas_threshold0.025_20251022_1609.csv\n",
      "  3Ô∏è‚É£  Ensemble Votaci√≥n: data/predicciones_kaggle_ENSEMBLE_VOTACION_5semillas_20251022_1611.csv\n",
      "  4Ô∏è‚É£  Ensemble Promedio: data/predicciones_kaggle_ENSEMBLE_PROMEDIO_5semillas_threshold0.025_20251022_1611.csv\n",
      "\n",
      "üí° RECOMENDACIONES:\n",
      "  ‚Ä¢ Correlaci√≥n moderada - ambos m√©todos de ensemble son v√°lidos\n",
      "  ‚Ä¢ Prueba ambos en Kaggle para ver cu√°l funciona mejor\n",
      "\n",
      "üéØ PR√ìXIMOS PASOS:\n",
      "  1. Subir los 4 archivos a Kaggle para comparar resultados\n",
      "  2. Revisar 'data/analisis_ensemble.json' para m√°s detalles\n",
      "  3. Considerar ajustar n_envios_ensemble seg√∫n performance en Kaggle\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ENSAMBLE DE MODELOS: EST√ÅNDAR + SLOPE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=\"*60)\n",
    "print(\"ENSAMBLE DE PREDICCIONES: EST√ÅNDAR + SLOPE\")\n",
    "print(\"=\"*60)\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================\n",
    "# ALINEAR DATASETS POR NUMERO_DE_CLIENTE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALINEANDO DATASETS POR NUMERO_DE_CLIENTE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Crear DataFrames con las predicciones\n",
    "df_pred_estandar = pd.DataFrame({\n",
    "    'numero_de_cliente': clientes_kaggle,\n",
    "    'prob_estandar': y_pred_prob_promedio,\n",
    "    'pred_estandar': y_pred_bin\n",
    "})\n",
    "\n",
    "df_pred_slope = pd.DataFrame({\n",
    "    'numero_de_cliente': clientes_kaggle_slope,\n",
    "    'prob_slope': y_pred_prob_promedio_slope,\n",
    "    'pred_slope': y_pred_bin_slope\n",
    "})\n",
    "\n",
    "print(f\"Registros modelo est√°ndar: {len(df_pred_estandar):,}\")\n",
    "print(f\"Registros modelo SLOPE: {len(df_pred_slope):,}\")\n",
    "\n",
    "# Hacer merge por numero_de_cliente\n",
    "df_ensemble = pd.merge(\n",
    "    df_pred_estandar,\n",
    "    df_pred_slope,\n",
    "    on='numero_de_cliente',\n",
    "    how='inner'  # Solo clientes que est√°n en ambos\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Registros despu√©s del merge: {len(df_ensemble):,}\")\n",
    "\n",
    "# Verificar si hay clientes que no matchean\n",
    "clientes_solo_estandar = set(clientes_kaggle) - set(clientes_kaggle_slope)\n",
    "clientes_solo_slope = set(clientes_kaggle_slope) - set(clientes_kaggle)\n",
    "\n",
    "if clientes_solo_estandar:\n",
    "    print(f\"‚ö†Ô∏è  Clientes solo en est√°ndar: {len(clientes_solo_estandar):,}\")\n",
    "if clientes_solo_slope:\n",
    "    print(f\"‚ö†Ô∏è  Clientes solo en SLOPE: {len(clientes_solo_slope):,}\")\n",
    "\n",
    "if len(df_ensemble) < len(df_pred_estandar):\n",
    "    print(f\"‚ö†Ô∏è  Atenci√≥n: Se perdieron {len(df_pred_estandar) - len(df_ensemble):,} registros en el merge\")\n",
    "\n",
    "# ============================================================\n",
    "# M√âTODO 1: ENSAMBLE POR VOTACI√ìN (UNI√ìN)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"M√âTODO 1: ENSAMBLE POR VOTACI√ìN (UNI√ìN)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Crear predicci√≥n por votaci√≥n: 1 si CUALQUIERA de los dos modelos predice 1\n",
    "df_ensemble['pred_votacion'] = (\n",
    "    (df_ensemble['pred_estandar'] == 1) | (df_ensemble['pred_slope'] == 1)\n",
    ").astype('int8')\n",
    "\n",
    "print(f\"\\nüìä ESTAD√çSTICAS DE VOTACI√ìN:\")\n",
    "print(f\"  Predicciones modelo est√°ndar (1): {df_ensemble['pred_estandar'].sum():,}\")\n",
    "print(f\"  Predicciones modelo SLOPE (1): {df_ensemble['pred_slope'].sum():,}\")\n",
    "print(f\"  Predicciones UNI√ìN (1): {df_ensemble['pred_votacion'].sum():,}\")\n",
    "print(f\"  % del total: {df_ensemble['pred_votacion'].sum() / len(df_ensemble) * 100:.2f}%\")\n",
    "\n",
    "# An√°lisis de overlap\n",
    "solo_estandar = ((df_ensemble['pred_estandar'] == 1) & (df_ensemble['pred_slope'] == 0)).sum()\n",
    "solo_slope = ((df_ensemble['pred_estandar'] == 0) & (df_ensemble['pred_slope'] == 1)).sum()\n",
    "ambos = ((df_ensemble['pred_estandar'] == 1) & (df_ensemble['pred_slope'] == 1)).sum()\n",
    "\n",
    "print(f\"\\nüîç AN√ÅLISIS DE OVERLAP:\")\n",
    "print(f\"  Solo modelo est√°ndar: {solo_estandar:,}\")\n",
    "print(f\"  Solo modelo SLOPE: {solo_slope:,}\")\n",
    "print(f\"  Ambos modelos: {ambos:,}\")\n",
    "print(f\"  Jaccard similarity: {ambos / df_ensemble['pred_votacion'].sum():.2%}\")\n",
    "\n",
    "# Crear submission por votaci√≥n\n",
    "submission_votacion = df_ensemble[['numero_de_cliente', 'pred_votacion']].copy()\n",
    "submission_votacion.columns = ['numero_de_cliente', 'Predicted']\n",
    "\n",
    "# Guardar\n",
    "fecha_votacion = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "filename_votacion = f\"data/predicciones_kaggle_ENSEMBLE_VOTACION_{len(SEMILLAS)}semillas_{fecha_votacion}.csv\"\n",
    "submission_votacion.to_csv(filename_votacion, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Archivo votaci√≥n guardado: '{filename_votacion}'\")\n",
    "\n",
    "# ============================================================\n",
    "# M√âTODO 2: ENSAMBLE POR PROMEDIO DE PROBABILIDADES\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"M√âTODO 2: ENSAMBLE POR PROMEDIO DE PROBABILIDADES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Promedio simple de ambas probabilidades\n",
    "df_ensemble['prob_ensemble'] = (df_ensemble['prob_estandar'] + df_ensemble['prob_slope']) / 2\n",
    "\n",
    "print(f\"Probabilidades ensemble:\")\n",
    "print(f\"  Prob min: {df_ensemble['prob_ensemble'].min():.6f}\")\n",
    "print(f\"  Prob max: {df_ensemble['prob_ensemble'].max():.6f}\")\n",
    "print(f\"  Prob media: {df_ensemble['prob_ensemble'].mean():.6f}\")\n",
    "\n",
    "# Guardar probabilidades ensemble\n",
    "np.save('data/predicciones/probabilidades_ensemble.npy', df_ensemble['prob_ensemble'].values)\n",
    "print(\"\\n‚úÖ Probabilidades ensemble guardadas\")\n",
    "\n",
    "# ============================================================\n",
    "# OPCI√ìN 2A: USAR THRESHOLD EN ENSEMBLE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n--- Opci√≥n 2A: Usar threshold en ensemble ---\")\n",
    "\n",
    "threshold_ensemble = 0.025\n",
    "print(f\"Threshold ensemble: {threshold_ensemble}\")\n",
    "\n",
    "# Binarizar seg√∫n threshold\n",
    "df_ensemble['pred_ensemble'] = (df_ensemble['prob_ensemble'] > threshold_ensemble).astype('int8')\n",
    "\n",
    "print(f\"Predicciones positivas: {df_ensemble['pred_ensemble'].sum():,}\")\n",
    "print(f\"% del total: {df_ensemble['pred_ensemble'].sum() / len(df_ensemble) * 100:.2f}%\")\n",
    "\n",
    "# Crear submission ensemble\n",
    "submission_ensemble = df_ensemble[['numero_de_cliente', 'pred_ensemble']].copy()\n",
    "submission_ensemble.columns = ['numero_de_cliente', 'Predicted']\n",
    "\n",
    "# Guardar\n",
    "fecha_ensemble = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "filename_ensemble = f\"data/predicciones_kaggle_ENSEMBLE_PROMEDIO_{len(SEMILLAS)}semillas_threshold{threshold_ensemble}_{fecha_ensemble}.csv\"\n",
    "submission_ensemble.to_csv(filename_ensemble, index=False)\n",
    "\n",
    "print(f\"‚úÖ Archivo ensemble guardado: '{filename_ensemble}'\")\n",
    "\n",
    "# ============================================================\n",
    "# OPCI√ìN 2B: PROBAR DIFERENTES THRESHOLDS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n--- Opci√≥n 2B: Generar m√∫ltiples versiones con diferentes thresholds ---\")\n",
    "\n",
    "thresholds_a_probar = [0.015, 0.020, 0.025, 0.030, 0.035, 0.040]\n",
    "\n",
    "for thresh in thresholds_a_probar:\n",
    "    print(f\"\\nGenerando ensemble con threshold {thresh}...\")\n",
    "    \n",
    "    df_temp = df_ensemble.copy()\n",
    "    df_temp['pred_temp'] = (df_temp['prob_ensemble'] > thresh).astype('int8')\n",
    "    \n",
    "    n_positivos = df_temp['pred_temp'].sum()\n",
    "    pct_positivos = n_positivos / len(df_temp) * 100\n",
    "    \n",
    "    submission_temp = df_temp[['numero_de_cliente', 'pred_temp']].copy()\n",
    "    submission_temp.columns = ['numero_de_cliente', 'Predicted']\n",
    "    \n",
    "    filename_temp = f\"data/predicciones/ensemble_promedio_threshold{thresh}.csv\"\n",
    "    submission_temp.to_csv(filename_temp, index=False)\n",
    "    print(f\"  ‚úÖ Guardado: {filename_temp} ({n_positivos:,} positivos - {pct_positivos:.2f}%)\")\n",
    "\n",
    "# ============================================================\n",
    "# RESUMEN COMPARATIVO DE TODOS LOS ENSAMBLES\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESUMEN COMPARATIVO DE ENSAMBLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "resumen_df = pd.DataFrame({\n",
    "    'M√©todo': [\n",
    "        'Modelo Est√°ndar',\n",
    "        'Modelo SLOPE',\n",
    "        'Ensemble Votaci√≥n (Uni√≥n)',\n",
    "        'Ensemble Promedio',\n",
    "    ],\n",
    "    'N¬∞ Env√≠os': [\n",
    "        df_ensemble['pred_estandar'].sum(),\n",
    "        df_ensemble['pred_slope'].sum(),\n",
    "        df_ensemble['pred_votacion'].sum(),\n",
    "        df_ensemble['pred_ensemble'].sum(),\n",
    "    ],\n",
    "    '% Base': [\n",
    "        f\"{df_ensemble['pred_estandar'].sum() / len(df_ensemble) * 100:.2f}%\",\n",
    "        f\"{df_ensemble['pred_slope'].sum() / len(df_ensemble) * 100:.2f}%\",\n",
    "        f\"{df_ensemble['pred_votacion'].sum() / len(df_ensemble) * 100:.2f}%\",\n",
    "        f\"{df_ensemble['pred_ensemble'].sum() / len(df_ensemble) * 100:.2f}%\",\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\")\n",
    "print(resumen_df.to_string(index=False))\n",
    "\n",
    "# ============================================================\n",
    "# AN√ÅLISIS DE CORRELACI√ìN ENTRE PROBABILIDADES\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AN√ÅLISIS DE CORRELACI√ìN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "correlacion = df_ensemble['prob_estandar'].corr(df_ensemble['prob_slope'])\n",
    "print(f\"Correlaci√≥n entre probabilidades (Est√°ndar vs SLOPE): {correlacion:.4f}\")\n",
    "\n",
    "if correlacion > 0.95:\n",
    "    print(\"  üìå Muy alta correlaci√≥n - los modelos son muy similares\")\n",
    "elif correlacion > 0.85:\n",
    "    print(\"  ‚úÖ Alta correlaci√≥n - modelos consistentes pero con diferencias\")\n",
    "elif correlacion > 0.70:\n",
    "    print(\"  üîÑ Correlaci√≥n moderada - buena diversidad para ensemble\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è  Correlaci√≥n baja - modelos muy diferentes (verificar)\")\n",
    "\n",
    "# ============================================================\n",
    "# VISUALIZACI√ìN DE DIFERENCIAS (OPCIONAL)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AN√ÅLISIS DE PREDICCIONES DIVERGENTES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calcular diferencias\n",
    "df_ensemble['diff_prob'] = abs(df_ensemble['prob_estandar'] - df_ensemble['prob_slope'])\n",
    "\n",
    "# Top 10 con mayor diferencia\n",
    "top_divergentes = df_ensemble.nlargest(10, 'diff_prob')\n",
    "\n",
    "print(f\"\\nTop 10 clientes con mayor divergencia en probabilidades:\")\n",
    "print(f\"{'Cliente':<20} {'Prob Est√°ndar':<15} {'Prob SLOPE':<15} {'Diferencia':<15}\")\n",
    "print(\"-\" * 65)\n",
    "for _, row in top_divergentes.iterrows():\n",
    "    print(f\"{row['numero_de_cliente']:<20} {row['prob_estandar']:<15.6f} {row['prob_slope']:<15.6f} {row['diff_prob']:<15.6f}\")\n",
    "\n",
    "# ============================================================\n",
    "# GUARDAR AN√ÅLISIS COMPLETO\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GUARDANDO AN√ÅLISIS COMPLETO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "analisis_completo = {\n",
    "    'fecha': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    'semillas_utilizadas': SEMILLAS.tolist() if hasattr(SEMILLAS, 'tolist') else list(SEMILLAS),\n",
    "    'registros_alineados': int(len(df_ensemble)),\n",
    "    'correlacion_modelos': float(correlacion),\n",
    "    'estadisticas': {\n",
    "        'modelo_estandar': {\n",
    "            'n_envios': int(df_ensemble['pred_estandar'].sum()),\n",
    "            'porcentaje': float(df_ensemble['pred_estandar'].sum() / len(df_ensemble) * 100)\n",
    "        },\n",
    "        'modelo_slope': {\n",
    "            'n_envios': int(df_ensemble['pred_slope'].sum()),\n",
    "            'porcentaje': float(df_ensemble['pred_slope'].sum() / len(df_ensemble) * 100)\n",
    "        },\n",
    "        'ensemble_votacion': {\n",
    "            'n_envios': int(df_ensemble['pred_votacion'].sum()),\n",
    "            'porcentaje': float(df_ensemble['pred_votacion'].sum() / len(df_ensemble) * 100),\n",
    "            'solo_estandar': int(solo_estandar),\n",
    "            'solo_slope': int(solo_slope),\n",
    "            'ambos': int(ambos)\n",
    "        },\n",
    "        'ensemble_promedio': {\n",
    "            'n_envios': int(df_ensemble['pred_ensemble'].sum()),\n",
    "            'porcentaje': float(df_ensemble['pred_ensemble'].sum() / len(df_ensemble) * 100)\n",
    "        }\n",
    "    },\n",
    "    'archivos_generados': {\n",
    "        'votacion': filename_votacion,\n",
    "        'promedio': filename_ensemble,\n",
    "        'estandar': filename,\n",
    "        'slope': filename_slope\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('data/analisis_ensemble.json', 'w') as f:\n",
    "    json.dump(analisis_completo, f, indent=4)\n",
    "\n",
    "print(\"‚úÖ An√°lisis guardado en 'data/analisis_ensemble.json'\")\n",
    "\n",
    "# ============================================================\n",
    "# RESUMEN FINAL\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=\"*60)\n",
    "print(\"üéâ PROCESO DE ENSEMBLE COMPLETADO\")\n",
    "print(\"=\"*60)\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìÅ ARCHIVOS GENERADOS:\")\n",
    "print(f\"  1Ô∏è‚É£  Modelo Est√°ndar: {filename}\")\n",
    "print(f\"  2Ô∏è‚É£  Modelo SLOPE: {filename_slope}\")\n",
    "print(f\"  3Ô∏è‚É£  Ensemble Votaci√≥n: {filename_votacion}\")\n",
    "print(f\"  4Ô∏è‚É£  Ensemble Promedio: {filename_ensemble}\")\n",
    "\n",
    "print(f\"\\nüí° RECOMENDACIONES:\")\n",
    "if correlacion > 0.90:\n",
    "    print(\"  ‚Ä¢ Alta correlaci√≥n entre modelos - el ensemble por promedio puede ser mejor\")\n",
    "    print(\"  ‚Ä¢ Considera usar el ensemble por promedio\")\n",
    "else:\n",
    "    print(\"  ‚Ä¢ Correlaci√≥n moderada - ambos m√©todos de ensemble son v√°lidos\")\n",
    "    print(\"  ‚Ä¢ Prueba ambos en Kaggle para ver cu√°l funciona mejor\")\n",
    "\n",
    "print(f\"\\nüéØ PR√ìXIMOS PASOS:\")\n",
    "print(f\"  1. Subir los 4 archivos a Kaggle para comparar resultados\")\n",
    "print(f\"  2. Revisar 'data/analisis_ensemble.json' para m√°s detalles\")\n",
    "print(f\"  3. Considerar ajustar n_envios_ensemble seg√∫n performance en Kaggle\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
